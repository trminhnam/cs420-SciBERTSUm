{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import bisect\n",
    "import subprocess\n",
    "import prepro.data_builder as data_builder\n",
    "from others.utils import clean\n",
    "import argparse\n",
    "from models.model_builder import ExtSummarizer\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-load_from\", default='../models/50.pt', type=str)\n",
    "parser.add_argument(\"-select_mode\", default='greedy', type=str)\n",
    "parser.add_argument(\"-shard_size\", default=50, type=int)\n",
    "parser.add_argument('-min_src_nsents', default=20, type=int)\n",
    "parser.add_argument('-max_src_nsents', default=500, type=int)\n",
    "parser.add_argument('-min_src_ntokens_per_sent', default=5, type=int)\n",
    "parser.add_argument('-max_src_ntokens_per_sent', default=50, type=int)\n",
    "parser.add_argument('-min_tgt_ntokens', default=50, type=int)\n",
    "parser.add_argument('-max_tgt_ntokens', default=5000, type=int)\n",
    "parser.add_argument(\"-lower\", type=str2bool, nargs='?', const=True, default=True)\n",
    "parser.add_argument(\"-use_bert_basic_tokenizer\", type=str2bool, nargs='?', const=True, default=False)\n",
    "parser.add_argument('-log_file', default='../logs/slide_gen.log')\n",
    "parser.add_argument(\"-large\", type=str2bool, nargs='?', const=True, default=False)\n",
    "parser.add_argument(\"-temp_dir\", default='../temp')\n",
    "parser.add_argument(\"-finetune_bert\", type=str2bool, nargs='?', const=True, default=False)\n",
    "parser.add_argument(\"-enc_hidden_size\", default=512, type=int)\n",
    "parser.add_argument(\"-enc_ff_size\", default=512, type=int)\n",
    "parser.add_argument(\"-enc_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-enc_layers\", default=6, type=int)\n",
    "parser.add_argument(\"-max_pos\", default=10240, type=int) #fix\n",
    "parser.add_argument(\"-chunk_size\", default=512, type=int) # fix\n",
    "parser.add_argument(\"-ext_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-ext_layers\", default=2, type=int, help=\"number of extractive encoder layers\")\n",
    "parser.add_argument(\"-ext_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-ext_heads\", default=4, type=int, help=\"number of attention head in each encoder layer\")\n",
    "parser.add_argument(\"-ext_ff_size\", default=2048, type=int)\n",
    "# global attention params\n",
    "parser.add_argument('-global_attention', default=1, type=int, choices=[0,1,2], help=\" global attention types:0,1,2. 0: no global attention, 1: global attention at random indices, 2: global attention at the beginning and end of the sections  \")\n",
    "parser.add_argument('-global_attention_ratio', default=0.2, type=float, help=\"ratio of global attention indices chosen at random\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "def load_model(args):\n",
    "    ## load checkpoint\n",
    "    checkpoint = torch.load(args.load_from, map_location=lambda storage, loc: storage)\n",
    "    model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers', 'encoder', 'ff_actv', 'use_interval', 'rnn_size']\n",
    "    opt = vars(checkpoint['opt'])\n",
    "    for k in opt.keys():\n",
    "        if k in model_flags:\n",
    "            setattr(args, k, opt[k])\n",
    "\n",
    "    ## load model\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # print(\"device:\", device)\n",
    "\n",
    "    model = ExtSummarizer(args, device, checkpoint)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def preprocess(pdf_path):\n",
    "    # Extract the sections from the PDF\n",
    "    outfile_path = os.path.join(args.temp_dir, \"paper.sections.txt\")\n",
    "    outfile = open(outfile_path, \"w\")\n",
    "    for line in data_builder.read_pdf_sections(pdf_path):\n",
    "        outfile.write(line.strip() + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "    # Run CoreNLP\n",
    "    command = ['java', 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize,ssplit',\n",
    "                'always', '-filelist', 'mapping_for_corenlp.txt', '-outputFormat',\n",
    "                'json', '-outputDirectory', args.temp_dir]\n",
    "\n",
    "    subprocess.call(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "    assert os.path.exists(outfile_path), \"CoreNLP failed to produce output file\"\n",
    "\n",
    "    # make data\n",
    "    pdf_json = os.path.join(args.temp_dir, \"paper.sections.txt.json\")\n",
    "    source = []\n",
    "    sections = []\n",
    "    section = 1\n",
    "    for sent in json.load(open(pdf_json))['sentences']:\n",
    "        tokens = [t['word'] for t in sent['tokens']]\n",
    "        after_tokens = [t['after'] for t in sent['tokens']]\n",
    "        if args.lower:\n",
    "            tokens = [t.lower() for t in tokens]\n",
    "        source.append(tokens)\n",
    "        sections.append(section)\n",
    "        if len(after_tokens) > 0 and after_tokens[-1] == '\\n':\n",
    "            section += 1\n",
    "\n",
    "    source = [clean(' '.join(sent)).split() for sent in source]\n",
    "\n",
    "    data_json = {'src': source, 'sections': sections}\n",
    "\n",
    "    # format_to_bert\n",
    "    corpus_type = \"test\"\n",
    "    json_f = os.path.join(args.temp_dir, \"data.json\")\n",
    "\n",
    "    def format_to_bert(args, corpus_type, data):\n",
    "        is_test = corpus_type == 'test'\n",
    "        bert = data_builder.BertData(args)\n",
    "        source, sections = data['src'], data['sections']\n",
    "        # greedily selects the top 3 sentences and labels them as 1\n",
    "        summary_size = int(0.2 * len(source))\n",
    "        if args.lower:\n",
    "            source = [' '.join(s).lower().split() for s in source]\n",
    "        b_data = bert.preprocess(source, sections, [], [],\n",
    "                                    use_bert_basic_tokenizer=args.use_bert_basic_tokenizer,\n",
    "                                    is_test=is_test)\n",
    "        src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt, sections, token_sections = b_data\n",
    "        b_data_dict = {\"src\": src_subtoken_idxs, \"tgt\": tgt_subtoken_idxs,\n",
    "                        \"src_sent_labels\": sent_labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "                        'src_txt': src_txt, \"tgt_txt\": tgt_txt, \"sections\": sections, \"token_sections\": token_sections}\n",
    "\n",
    "        return b_data_dict\n",
    "\n",
    "    data = format_to_bert(args, corpus_type, data_json)\n",
    "    return data\n",
    "\n",
    "def post_process(data, device):\n",
    "    src = data['src']\n",
    "    sections = data['sections']\n",
    "    token_sections = data['token_sections']\n",
    "    segs = data['segs']\n",
    "    clss = data['clss']\n",
    "\n",
    "    src_txt = data['src_txt']\n",
    "\n",
    "    end_id = [src[-1]]\n",
    "    lastIsCls = False\n",
    "    if len(src) > args.max_pos-1 and src[args.max_pos-1] == 101:\n",
    "        lastIsCls = True\n",
    "\n",
    "    src = src[:-1][:args.max_pos - 1] + end_id\n",
    "    segs = segs[:args.max_pos]\n",
    "\n",
    "    token_sections = token_sections[:args.max_pos]\n",
    "    max_sent_id = bisect.bisect_left(clss, args.max_pos)\n",
    "    clss = clss[:max_sent_id]\n",
    "    sections = sections[:max_sent_id]\n",
    "    if lastIsCls:\n",
    "        clss = clss[:max_sent_id-1]\n",
    "        sections = sections[: max_sent_id-1]\n",
    "\n",
    "    def _pad(data, pad_id, width=-1):\n",
    "        if width == -1:\n",
    "            width = max(len(d) for d in data)\n",
    "        rtn_data = [d + [pad_id] * (width - len(d)) for d in data]\n",
    "        return rtn_data\n",
    "\n",
    "    src = torch.tensor(_pad([src], 0)).to(int)\n",
    "    segs = torch.tensor(_pad([segs], 0)).to(int)\n",
    "    token_sections = torch.tensor(_pad([token_sections], 0)).to(int)\n",
    "    clss = torch.tensor(_pad([clss], -1)).to(int)\n",
    "    sections = torch.tensor(_pad([sections], 0)).to(int)\n",
    "    mask_src = ~ (src == 0).to(int)\n",
    "    mask_cls = ~ (clss == -1)\n",
    "    clss[clss == -1] = 0\n",
    "\n",
    "    # move to device\n",
    "    src = src.to(device)\n",
    "    segs = segs.to(device)\n",
    "    token_sections = token_sections.to(device)\n",
    "    mask_src = mask_src.to(device)\n",
    "    clss = clss.to(device)\n",
    "    sections = sections.to(device)\n",
    "    mask_cls = mask_cls.to(device)\n",
    "\n",
    "    return {\n",
    "        'src': src, \n",
    "        'segs': segs, \n",
    "        'clss': clss,\n",
    "        'sections': sections, \n",
    "        'token_sections': token_sections, \n",
    "        'mask_src': mask_src, \n",
    "        'mask_cls': mask_cls\n",
    "    }, src_txt\n",
    "\n",
    "def predict(args, model, pdf_path, device):\n",
    "\n",
    "    extracted_data = preprocess(pdf_path)\n",
    "    data, src_txt = post_process(extracted_data, device)\n",
    "    mask_cls = data['mask_cls']\n",
    "    sent_scores, mask = model(**data)\n",
    "\n",
    "    batch_size, sent_count = mask_cls.shape\n",
    "    sent_scores = sent_scores[:, :sent_count]  # remove padded items from returned scores\n",
    "\n",
    "    sent_scores = sent_scores.cpu().data.numpy()\n",
    "    selected_ids = np.argsort(-sent_scores, 1)[0]\n",
    "\n",
    "    # sort src text by section\n",
    "    src_txt = [src_txt[i] for i in selected_ids]\n",
    "\n",
    "    return selected_ids, src_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../examples_data/Paper_cf.tei.xml\"\n",
    "selected_ids, src_txt = predict(args, model, pdf_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n"
     ]
    }
   ],
   "source": [
    "from grobid_client.grobid_client import GrobidClient\n",
    "\n",
    "client = GrobidClient(config_path=\"./config.json\")\n",
    "\n",
    "pdf_dir = \"../demo/Paper_cf.pdf\"\n",
    "client.process(\"processFulltextDocument\", pdf_dir, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zalo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e682177e4c24e3a44aae13c1fdbe5f10388744bb58fe6e460fcd3e59003f845d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
