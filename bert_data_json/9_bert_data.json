{"src": [101, 1996, 3259, 8908, 1996, 9366, 1997, 7399, 4730, 12992, 2075, 2000, 5047, 17837, 2951, 13462, 2015, 1012, 102, 101, 4866, 7885, 2007, 3793, 5579, 3291, 12826, 1996, 2836, 1997, 1037, 2193, 1997, 2367, 12992, 2075, 9942, 1010, 16966, 2006, 1996, 3471, 13686, 2011, 17837, 2951, 13462, 2015, 1012, 102, 101, 12992, 2075, 2003, 1037, 4118, 1997, 11566, 2061, 1011, 2170, 5410, 26262, 2008, 14258, 2069, 4685, 3621, 2488, 2084, 1037, 6721, 2465, 18095, 2046, 1037, 18215, 5257, 2008, 2465, 14144, 2007, 2152, 10640, 1012, 102, 101, 1999, 2236, 12992, 2075, 2038, 2042, 3491, 2000, 8327, 1037, 9487, 5012, 2000, 2058, 8873, 13027, 1012, 102, 101, 2019, 7526, 2005, 2023, 9575, 6083, 2008, 2023, 3463, 2013, 12992, 2075, 23569, 27605, 7741, 1996, 7785, 1997, 1996, 10318, 18215, 5257, 1997, 5410, 26262, 1031, 2184, 1033, 1012, 102, 101, 2023, 7613, 2038, 4081, 1037, 2193, 1997, 12719, 1997, 1996, 10318, 12992, 2075, 5656, 2083, 5278, 1996, 5468, 1997, 1996, 7785, 4353, 2008, 2003, 23569, 27605, 6924, 1031, 1022, 1033, 1012, 102, 101, 2635, 1037, 1015, 1011, 13373, 1997, 1996, 19840, 10857, 1998, 23569, 27605, 7741, 1996, 1015, 1011, 13373, 1997, 1996, 21374, 5260, 2000, 1037, 7399, 4746, 1012, 102, 101, 2065, 2023, 2003, 13332, 2011, 2061, 1011, 2170, 5930, 4245, 1010, 1996, 4525, 9896, 2064, 2022, 2464, 2004, 1037, 12992, 2075, 9896, 1031, 1016, 29623, 2509, 1033, 1010, 2073, 1996, 22289, 5576, 3957, 1996, 3635, 8613, 1997, 1996, 5410, 26262, 1998, 1996, 7037, 5576, 1996, 20611, 2058, 4973, 29625, 10760, 3160, 1997, 2129, 2000, 15581, 12992, 2075, 102, 101, 2027, 3107, 1996, 1057, 29624, 5092, 14122, 9896, 2008, 25352, 1996, 4353, 3635, 2075, 1999, 7927, 1997, 1996, 4973, 2013, 1996, 3760, 2465, 29625, 10760, 6614, 1997, 1996, 2783, 3259, 2003, 2000, 8970, 1037, 2544, 1997, 1996, 7399, 4730, 12992, 2075, 2008, 2003, 15757, 2005, 17837, 2951, 13462, 2015, 1012, 102, 101, 2023, 2003, 1037, 3019, 3921, 2000, 8304, 17837, 2951, 13462, 2015, 2004, 1996, 9896, 23569, 27605, 8583, 1037, 3465, 19229, 2008, 2064, 2022, 5967, 2000, 8339, 1996, 17837, 2791, 1997, 1996, 2951, 13462, 29625, 4017, 1996, 2168, 2051, 2057, 2556, 4866, 7885, 13599, 1037, 2193, 1997, 2367, 12992, 2075, 9942, 1998, 4725, 1997, 5410, 4553, 2121, 4245, 1012, 102, 101, 2256, 2364, 3231, 2793, 2005, 1996, 6388, 2147, 2003, 1996, 26665, 6254, 3074, 1012, 102, 101, 1999, 2023, 2930, 2057, 8970, 1996, 12992, 2075, 13792, 2641, 1999, 2256, 7885, 1010, 2164, 1996, 6789, 1997, 1996, 7399, 4730, 9896, 2005, 17837, 2951, 13462, 2015, 29625, 5092, 14122, 2075, 2003, 1037, 4118, 2000, 2424, 1037, 3811, 8321, 5579, 3627, 2011, 11566, 2116, 5410, 2030, 2918, 1044, 22571, 14573, 23072, 1012, 102, 101, 2169, 5410, 10744, 2089, 2022, 2069, 17844, 8321, 1012, 102, 101, 5410, 26262, 2024, 4738, 25582, 2135, 1012, 102, 101, 2169, 5410, 4553, 2121, 2003, 4738, 2006, 4973, 2029, 1996, 11003, 5410, 26262, 2179, 2087, 3697, 2000, 26268, 1012, 102, 101, 2292, 1055, 1027, 1006, 1060, 1015, 1010, 1061, 1015, 1007, 1010, 1012, 1012, 1012, 1010, 1006, 1060, 1049, 1010, 1061, 1049, 1007, 2022, 1037, 2275, 1997, 2731, 4973, 1012, 102, 101, 2169, 6013, 1060, 1045, 7460, 2000, 1037, 5884, 1060, 1998, 2038, 4137, 1037, 2309, 2465, 1061, 1045, 1012, 102, 101, 2169, 2465, 1061, 1045, 7460, 2000, 1037, 10713, 3830, 2686, 1061, 1012, 102, 101, 1999, 2023, 3259, 2057, 3579, 2006, 12441, 5579, 3471, 1999, 2029, 1061, 1027, 1063, 1597, 1015, 1010, 1009, 2487, 1065, 1012, 102, 101, 2057, 2655, 4973, 2383, 1061, 1045, 1027, 1009, 2487, 3893, 1998, 2216, 2383, 1061, 1045, 1027, 1597, 1015, 4997, 4973, 29625, 2050, 5410, 4083, 9896, 13385, 2004, 7953, 1037, 5537, 1997, 2731, 4973, 1055, 1998, 1037, 4353, 1040, 1056, 1006, 2073, 1040, 1056, 1006, 1045, 1007, 2071, 2022, 10009, 2004, 1996, 28616, 26266, 9031, 3465, 102, 101, 2241, 2006, 2023, 7953, 1996, 5410, 4553, 2121, 27852, 1037, 5410, 10744, 1044, 1012, 102, 101, 2057, 17841, 1996, 3696, 1997, 1044, 1006, 1060, 1007, 2004, 1996, 10173, 3830, 1998, 1996, 10194, 1064, 1044, 1006, 1060, 1007, 1064, 2004, 1996, 7023, 1999, 2008, 17547, 1012, 102, 101, 1996, 16381, 1155, 1056, 2003, 4217, 2000, 5468, 1996, 5197, 8931, 1997, 1996, 9086, 2248, 3034, 2006, 3698, 4083, 1006, 24582, 19968, 1011, 2494, 1007, 1010, 2899, 5887, 1010, 2494, 1012, 102, 101, 2445, 1156, 1998, 1037, 2731, 2275, 1024, 1055, 1027, 1006, 1060, 1015, 1010, 1061, 1015, 1007, 1010, 1012, 1012, 1012, 1010, 1006, 1060, 1049, 1010, 1061, 1049, 1007, 2073, 2595, 1045, 1596, 1060, 1010, 1061, 1045, 1596, 1063, 1597, 1015, 1010, 1009, 2487, 1065, 3988, 5562, 1024, 1040, 1015, 1006, 1045, 1007, 1027, 102, 101, 1037, 2691, 11967, 2043, 4083, 2006, 10047, 26657, 2094, 2951, 4520, 2003, 2008, 1996, 4738, 2465, 18095, 2465, 14144, 2035, 4973, 2000, 1996, 2350, 2465, 1012, 102, 101, 1999, 3793, 5579, 2057, 2411, 8087, 2023, 3291, 2043, 2057, 2024, 4083, 1037, 12441, 2465, 18095, 2000, 3584, 1037, 2235, 8476, 2013, 1996, 2717, 1997, 1996, 5491, 1012, 102, 101, 1999, 2344, 2000, 4468, 2023, 3291, 1010, 2057, 2052, 2066, 2000, 7902, 2063, 1996, 5197, 1997, 1996, 3760, 2465, 1012, 102, 101, 2057, 7868, 2008, 1996, 3760, 2465, 2003, 1996, 3893, 2465, 2096, 1996, 7444, 2465, 2003, 1996, 4997, 2028, 29625, 8447, 12083, 9541, 3367, 1031, 1021, 1033, 21183, 24411, 2229, 2087, 1997, 1996, 4784, 1997, 15262, 5092, 14122, 1031, 2340, 1033, 1010, 2021, 13999, 2019, 16655, 26426, 3279, 3853, 1998, 6310, 3635, 2039, 16616, 3627, 1012, 102, 101, 1996, 3601, 1997, 1037, 16381, 1156, 2097, 2486, 17837, 28616, 26266, 9031, 5366, 2005, 2731, 4973, 1006, 3275, 1015, 1007, 1012, 102, 101, 4973, 2013, 1996, 3760, 1010, 3893, 1010, 2465, 2097, 3322, 2131, 4137, 1156, 2335, 3469, 15871, 2084, 4997, 4973, 1012, 102, 101, 1996, 16381, 1156, 2097, 2036, 5009, 1996, 3635, 2039, 16616, 3627, 2000, 3623, 1996, 3635, 1997, 6270, 4997, 2015, 2062, 24663, 2084, 1997, 6270, 3893, 2015, 1012, 102, 101, 2006, 1996, 2060, 2192, 2009, 2097, 9885, 1996, 15871, 1997, 2995, 3893, 2015, 2062, 4603, 2135, 2084, 1997, 2995, 4997, 2015, 1012, 102, 101, 2104, 2023, 2039, 16616, 3627, 1996, 15871, 1997, 3893, 4973, 4050, 5441, 3020, 5300, 1012, 102, 101, 2169, 5410, 10744, 3568, 12102, 2000, 11178, 26268, 2062, 3893, 4973, 1010, 2144, 2027, 5441, 3020, 15871, 1012, 102, 101, 1996, 2345, 10744, 1010, 1037, 7399, 5257, 1997, 1996, 5410, 1044, 22571, 14573, 23072, 1010, 2097, 2036, 11178, 26268, 2062, 3893, 4973, 1012, 102, 101, 6948, 5092, 14122, 1031, 1019, 1033, 2003, 1037, 7399, 2565, 1006, 6948, 1007, 3921, 2000, 12992, 2075, 1012, 102, 101, 1996, 3259, 1031, 1019, 1033, 3065, 2008, 2635, 1037, 1015, 1011, 13373, 1997, 1996, 19840, 10857, 1998, 23569, 27605, 7741, 1996, 1015, 1011, 13373, 1997, 1996, 21374, 5260, 2000, 1037, 7399, 4746, 1012, 102, 101, 2065, 2023, 2003, 13332, 2011, 2061, 1011, 2170, 5930, 4245, 1010, 1996, 4525, 9896, 2064, 2022, 2464, 2004, 1037, 12992, 2075, 9896, 1031, 1016, 29623, 2509, 1033, 1010, 2073, 1996, 22289, 7300, 2507, 1996, 3635, 8613, 1997, 1996, 5410, 26262, 1998, 1996, 7037, 7300, 1996, 20611, 2058, 4973, 29625, 14277, 5092, 14122, 2009, 25284, 2135, 23569, 27605, 8583, 7037, 28616, 26266, 9031, 5366, 1998, 102, 101, 1999, 5688, 2000, 17978, 12992, 2075, 13792, 1010, 2029, 2089, 2069, 28314, 1999, 1996, 5787, 1010, 6948, 5092, 14122, 28314, 2015, 1999, 1037, 10713, 2193, 1997, 27758, 2015, 2000, 1037, 16452, 15502, 5576, 17087, 2092, 1011, 4225, 15502, 3012, 3785, 29625, 5643, 2052, 5987, 6948, 5092, 14122, 2000, 2022, 15078, 2135, 6450, 1012, 102, 101, 2057, 2179, 1010, 2174, 1010, 2008, 2019, 27758, 1997, 6948, 5092, 14122, 2003, 3621, 2062, 6450, 2084, 2019, 27758, 1997, 15262, 5092, 14122, 1010, 2021, 2006, 1996, 2060, 2192, 6948, 5092, 14122, 3791, 2521, 8491, 27758, 2015, 2084, 15262, 5092, 14122, 2000, 28314, 1012, 102, 101, 1996, 6948, 5092, 14122, 9896, 1031, 1019, 1033, 2003, 12774, 2013, 1037, 2236, 6648, 4106, 2008, 19202, 1996, 3231, 7561, 1999, 3408, 1997, 1996, 1015, 1011, 13373, 1997, 1996, 9207, 1997, 21374, 1010, 7785, 4719, 1998, 1996, 19840, 10857, 1012, 102, 101, 1996, 7163, 15630, 3370, 1997, 1996, 5391, 5260, 2000, 1037, 7399, 4746, 2008, 3957, 1996, 20219, 2682, 2043, 4991, 2000, 1996, 7037, 29625, 8545, 2085, 2507, 1037, 2714, 14354, 2005, 1996, 17837, 2544, 1997, 1996, 6948, 5092, 14122, 9896, 1010, 2057, 2031, 2315, 6948, 12083, 9541, 3367, 1012, 102, 101, 1999, 2023, 2553, 2057, 7868, 2008, 1996, 3465, 1997, 28616, 26266, 11787, 1037, 3893, 2742, 1006, 10262, 2008, 1996, 3893, 2465, 2003, 1996, 2625, 10357, 1007, 2003, 3469, 2084, 2008, 1997, 28616, 26266, 11787, 1037, 4997, 2742, 1012, 102, 101, 6516, 1010, 1996, 3279, 1048, 1042, 3378, 2007, 1037, 5579, 3853, 1042, 2003, 2445, 2011, 2140, 1042, 1006, 1060, 1010, 1061, 1007, 1027, 1156, 2361, 1006, 1042, 1006, 1060, 1007, 1608, 1014, 1998, 1061, 1027, 1015, 1007, 1009, 1052, 1006, 1042, 1006, 1060, 1007, 1609, 1014, 1998, 1061, 1027, 1597, 1015, 1007, 102, 101, 2057, 2064, 3356, 5391, 2023, 3279, 2004, 4076, 2140, 1042, 1006, 1060, 1010, 1061, 1007, 1608, 1156, 1006, 1015, 1009, 1061, 1007, 1016, 29732, 1006, 1171, 1597, 1042, 1006, 1060, 1007, 1007, 1009, 1009, 1015, 1597, 1061, 1016, 29732, 1006, 1042, 1006, 1060, 1007, 1009, 1171, 1007, 1009, 1010, 1006, 1015, 1007, 2073, 102, 101, 2057, 2064, 2085, 6611, 1996, 10958, 3207, 22911, 5886, 6028, 1031, 1015, 1033, 2445, 1156, 1998, 1037, 2731, 2275, 1024, 1055, 1027, 1006, 1060, 1015, 1010, 1061, 1015, 1007, 1010, 1012, 1012, 1012, 1010, 1006, 1060, 1049, 1010, 1061, 1049, 1007, 2073, 2000, 5391, 1996, 3279, 1999, 3408, 1997, 2049, 17537, 3643, 1998, 102, 101, 1996, 10958, 3207, 22911, 5886, 11619, 1997, 1037, 2465, 2515, 2025, 3623, 2065, 2057, 2693, 2000, 2049, 18309, 6738, 1998, 2061, 1996, 5391, 7336, 1996, 1015, 1011, 13373, 1997, 1996, 19840, 10857, 18953, 2011, 1996, 19262, 7785, 4606, 1996, 10958, 3207, 22911, 5886, 11619, 1997, 1996, 5410, 26262, 2153, 18953, 2011, 1996, 19262, 7785, 1012, 102, 101, 2057, 18168, 4183, 1996, 4751, 2138, 1997, 2686, 14679, 1012, 102, 101, 1996, 4525, 5391, 2003, 23569, 27605, 6924, 2011, 1996, 2206, 7399, 4746, 1024, 2005, 6948, 12083, 9541, 3367, 2057, 2202, 1996, 2168, 6948, 20219, 1997, 12992, 2075, 2004, 1999, 6948, 5092, 14122, 1010, 2021, 2057, 8970, 1037, 2047, 16381, 1156, 2383, 1996, 2168, 2535, 2004, 1999, 15262, 12083, 9541, 3367, 1012, 102, 101, 3893, 4973, 2031, 1037, 1156, 2335, 3020, 5391, 2006, 2037, 28616, 26266, 9031, 5366, 2595, 1045, 1596, 1060, 1010, 1061, 1045, 1596, 1063, 1597, 1015, 1010, 1009, 2487, 1065, 3988, 5562, 1024, 1155, 1027, 1014, 1010, 1050, 1027, 1014, 1010, 1156, 6948, 1027, 1014, 1057, 1045, 1027, 1059, 1009, 2065, 1061, 1045, 1027, 1009, 2487, 1059, 102, 101, 1057, 1045, 1006, 1040, 1009, 1040, 1597, 1027, 1040, 1009, 1040, 1597, 1027, 1156, 1007, 1012, 102, 101, 1996, 3601, 1997, 16381, 1040, 6053, 7711, 1996, 2896, 5391, 2006, 1996, 28616, 26266, 9031, 3465, 1057, 1045, 1012, 102, 101, 2057, 2071, 2275, 2009, 2000, 1014, 1010, 2021, 2057, 2738, 2275, 1040, 1597, 1027, 1015, 1049, 16177, 1010, 2073, 1167, 1596, 1006, 1014, 1010, 1015, 1007, 1010, 1998, 2094, 1597, 1040, 1597, 1027, 1040, 1009, 1040, 1009, 1027, 1040, 6053, 1012, 102, 101, 1996, 3291, 2057, 5159, 2003, 1996, 14639, 1997, 6948, 1998, 6948, 2226, 12992, 2000, 1996, 3643, 1997, 16381, 1167, 1010, 2029, 2038, 2000, 2022, 15757, 2007, 2070, 2729, 2000, 6855, 1037, 2204, 19143, 3446, 29625, 14277, 12083, 9541, 3367, 13585, 15262, 12083, 9541, 3367, 2383, 2019, 17837, 3279, 3853, 1998, 6948, 5092, 14122, 2383, 1037, 2092, 1011, 4225, 7458, 19229, 2007, 102, 101, 2023, 3084, 6948, 12083, 9541, 3367, 1037, 2204, 9896, 2005, 4083, 2006, 17837, 2731, 4520, 1024, 8498, 3020, 15871, 2006, 3893, 4973, 2096, 2478, 2019, 6948, 2000, 6855, 2019, 15502, 5257, 1997, 5410, 1044, 22571, 14573, 23072, 2007, 4847, 2000, 1037, 2092, 1011, 12774, 23569, 27605, 26652, 19229, 1012, 102, 101, 1037, 5410, 4083, 9896, 2003, 1037, 7709, 2005, 9798, 5410, 1044, 22571, 14573, 23072, 1012, 102, 101, 12992, 2075, 4858, 1037, 2275, 1997, 5410, 1044, 22571, 14573, 23072, 2011, 8385, 4214, 1037, 5410, 4083, 9896, 1012, 102, 101, 1996, 5410, 1044, 22571, 14573, 23072, 2024, 7399, 2135, 4117, 2046, 1037, 2309, 3627, 1012, 102, 101, 1996, 7953, 1997, 1996, 5410, 4083, 9896, 2003, 1037, 4353, 2030, 9207, 1997, 15871, 1006, 28616, 26266, 9031, 5366, 1007, 1010, 1998, 1037, 2731, 2951, 2275, 1012, 102, 101, 5410, 4083, 13792, 2224, 1996, 15871, 2000, 2424, 1037, 5410, 10744, 2029, 2038, 1037, 17844, 2659, 7561, 2007, 4847, 2000, 1996, 15871, 29625, 20041, 2000, 1996, 3635, 2039, 16616, 3627, 4973, 2029, 2024, 2524, 2000, 26268, 2097, 2131, 4297, 28578, 21050, 2135, 3020, 15871, 2096, 1996, 4973, 3733, 2000, 26268, 2131, 2896, 15871, 1012, 102, 101, 1996, 3466, 2003, 2000, 2486, 1996, 4745, 5410, 26262, 2000, 10152, 2006, 2524, 1011, 2000, 1011, 26268, 4973, 1012, 102, 101, 12992, 2075, 2003, 1037, 2236, 3800, 4118, 2008, 2064, 2022, 4117, 2007, 2151, 5410, 4553, 2121, 1012, 102, 101, 1999, 3218, 2009, 2038, 2042, 4117, 2007, 1037, 2898, 3528, 1997, 4280, 2164, 3247, 3628, 1998, 15756, 6125, 29625, 2378, 2023, 3259, 2057, 3579, 2006, 12992, 2075, 2478, 2200, 3722, 2465, 28295, 1012, 102, 101, 2035, 2465, 28295, 2641, 1999, 2023, 2930, 2031, 1996, 2433, 1997, 1037, 2028, 2504, 3247, 3392, 1006, 2065, 1011, 2059, 3627, 1007, 1012, 102, 101, 1996, 3231, 2003, 2000, 4638, 2005, 1996, 3739, 1997, 1037, 2773, 1999, 1037, 2445, 6254, 1012, 102, 101, 2241, 2006, 1996, 3739, 1997, 1996, 2773, 1996, 5410, 10744, 27852, 1037, 17547, 1012, 102, 101, 2057, 17841, 1996, 3696, 1997, 1996, 17547, 2004, 1037, 10173, 2465, 1006, 9131, 2057, 2024, 7149, 2007, 12441, 3471, 1007, 1998, 1996, 10194, 1997, 1996, 6434, 2004, 1996, 7023, 1999, 2008, 17547, 29625, 29278, 2742, 1024, 2065, 2057, 3046, 2000, 16014, 2029, 5491, 7141, 2000, 1996, 2998, 4696, 1010, 2057, 2097, 3345, 102, 101, 2059, 2256, 5410, 10744, 2071, 2022, 1037, 3627, 1024, 1000, 2065, 1996, 2773, 2374, 5158, 1999, 1037, 6254, 1010, 2059, 2057, 2024, 3811, 9657, 1996, 6254, 7460, 2000, 2998, 4696, 1012, 102, 101, 2006, 1996, 2060, 2192, 1010, 2065, 2374, 2515, 2025, 5258, 1999, 1037, 6254, 2059, 2057, 16014, 1996, 6254, 2515, 2025, 7141, 2000, 1996, 2998, 4696, 2007, 2659, 7023, 1012, 102, 101, 1000, 6246, 1010, 2057, 4339, 1059, 1596, 1060, 2000, 2812, 1037, 2744, 1059, 5158, 1999, 6254, 1060, 1012, 102, 101, 2061, 2057, 9375, 1037, 5410, 1044, 22571, 14573, 23072, 1044, 2029, 3084, 20932, 1024, 1044, 1006, 1060, 1007, 1027, 1039, 1009, 2065, 1059, 1596, 1060, 1039, 1597, 2065, 1059, 1013, 1596, 1060, 1006, 1016, 1007, 2073, 1039, 1009, 1998, 1039, 1597, 2024, 2613, 3616, 29625, 7485, 2149, 2036, 9375, 1024, 2445, 1037, 2783, 4353, 1040, 102, 101, 2005, 2169, 2744, 1010, 1996, 5300, 1039, 1009, 1998, 1039, 1597, 1998, 1037, 3556, 2024, 4137, 2000, 2008, 3327, 5410, 4553, 2121, 1012, 102, 101, 2044, 2035, 3408, 2031, 2042, 9022, 1010, 1037, 4553, 2121, 2007, 2190, 3556, 2003, 4217, 1012, 102, 101, 2367, 5410, 26262, 2224, 2367, 7644, 29625, 29278, 15262, 5092, 14122, 29625, 2213, 2232, 1037, 5391, 2006, 17537, 3279, 1006, 12884, 1997, 28616, 26266, 7810, 4973, 1007, 2038, 2042, 10003, 2011, 8040, 3270, 20781, 1998, 3220, 1031, 2340, 1033, 1012, 102, 101, 2027, 3662, 2008, 1996, 10654, 6562, 3279, 1006, 1049, 2232, 4832, 2005, 6263, 10654, 6562, 3279, 1007, 1997, 1996, 28043, 3853, 4663, 2478, 15262, 5092, 14122, 29625, 2213, 2232, 2003, 2012, 2087, 1056, 1056, 1027, 1015, 1062, 1056, 1010, 2073, 1062, 1056, 2003, 1037, 3671, 3989, 5387, 2012, 2461, 1056, 1012, 102, 101, 2023, 3356, 5391, 2064, 2022, 1037, 2109, 2004, 1037, 5009, 4179, 2005, 10549, 1155, 1056, 1998, 1996, 2640, 1997, 1996, 5410, 4083, 9896, 1012, 102, 101, 1996, 2034, 9896, 2003, 2170, 15262, 5092, 14122, 29625, 2213, 2232, 1006, 2613, 29625, 2213, 2232, 1007, 2007, 2613, 1011, 11126, 20932, 1031, 2260, 1033, 1012, 102, 101, 2057, 9146, 1039, 1009, 1998, 1039, 1597, 2000, 2022, 16591, 20623, 2613, 11126, 20932, 29625, 4183, 2001, 3491, 1999, 1031, 2340, 1033, 2008, 1062, 1056, 2003, 7163, 28732, 2094, 2011, 10549, 2278, 1038, 1027, 1015, 1016, 1048, 2078, 1059, 1038, 1009, 1059, 1038, 1597, 1006, 1018, 1007, 1998, 4292, 1155, 1056, 1027, 1015, 12748, 2008, 1024, 1062, 102, 101, 2004, 4081, 1999, 8040, 3270, 20781, 1998, 3220, 1031, 2260, 1033, 2057, 5744, 1996, 5300, 1997, 1039, 1038, 2000, 5787, 1996, 10194, 2015, 1997, 20932, 2278, 1038, 1027, 1015, 1016, 1048, 2078, 1059, 1038, 1009, 1009, 1059, 1038, 1597, 1009, 1006, 1020, 1007, 2057, 2224, 1027, 1015, 1049, 1012, 102, 101, 2144, 1059, 1038, 1009, 1998, 1059, 1038, 1597, 1596, 1031, 1014, 1010, 1015, 1033, 1010, 2023, 19202, 1064, 1039, 1038, 1064, 2011, 5560, 1015, 1016, 1048, 2078, 1006, 1015, 1013, 1013, 1007, 1012, 102, 101, 15262, 5092, 14122, 29625, 2213, 2232, 2007, 16246, 20932, 1006, 5860, 29625, 2213, 2232, 1007, 2749, 1996, 20932, 1039, 1038, 1997, 1996, 5410, 10744, 2000, 2022, 2593, 1009, 2487, 2030, 1597, 1015, 1012, 102, 101, 2023, 2003, 1037, 2062, 3151, 4292, 2073, 20932, 2079, 2025, 4287, 7023, 2015, 1010, 1998, 1155, 1056, 2003, 1037, 5468, 1997, 7023, 1999, 1996, 5410, 10744, 1012, 102, 101, 2057, 2145, 7163, 28732, 1062, 1056, 2005, 1037, 2445, 2744, 1059, 29625, 2478, 1996, 2168, 14869, 2004, 3107, 1999, 1996, 3025, 2930, 1010, 2057, 2275, 1024, 1039, 1038, 1027, 3696, 1006, 1059, 1038, 1009, 1597, 1059, 1038, 1597, 1007, 1006, 1021, 1007, 2057, 2064, 17841, 1996, 3601, 1997, 1039, 1038, 2004, 1037, 102, 101, 2292, 1054, 1056, 1027, 1064, 1059, 1009, 1009, 1597, 1059, 1009, 1597, 1064, 1009, 1064, 1059, 1597, 1009, 1597, 1059, 1597, 1597, 1064, 1010, 2059, 2009, 2064, 2022, 3491, 1031, 2340, 1033, 1010, 2008, 1999, 2344, 2000, 7163, 28732, 1062, 1056, 2057, 2323, 5454, 1024, 1155, 1056, 1027, 1015, 1016, 1048, 2078, 102, 101, 2061, 2057, 5454, 1037, 5410, 10744, 1006, 1037, 2744, 1059, 1007, 2029, 2038, 1996, 10479, 1062, 1056, 1012, 102, 101, 1996, 6948, 5092, 14122, 9896, 1031, 1019, 1033, 1006, 2156, 2930, 1016, 29625, 2509, 1007, 6083, 2008, 2012, 2169, 2461, 1997, 12992, 2075, 1037, 5410, 10744, 1044, 2007, 29160, 7680, 1997, 28616, 26266, 9031, 5366, 1040, 1056, 28608, 2011, 2465, 3643, 1061, 1045, 1596, 1063, 1597, 1015, 1010, 1009, 2487, 1065, 1998, 17547, 1044, 1024, 26718, 17421, 1027, 1049, 102, 101, 2144, 2023, 2003, 1037, 2367, 19229, 2005, 10549, 1996, 2190, 5410, 1011, 10744, 1999, 15262, 5092, 14122, 29625, 2213, 2232, 1010, 2057, 6855, 2048, 2047, 5410, 26262, 1012, 102, 101, 2057, 2655, 2068, 2613, 29625, 14277, 1998, 5860, 29625, 14277, 1012, 102, 101, 2027, 11234, 2013, 15262, 5092, 14122, 29625, 2213, 2232, 1999, 1996, 2126, 5410, 10744, 2003, 4217, 1010, 2612, 1997, 7163, 15630, 2075, 1062, 1056, 1010, 26718, 17421, 2003, 20446, 5084, 1012, 102, 101, 13173, 24861, 8523, 1998, 8233, 2063, 1011, 4202, 1999, 2037, 3259, 2006, 12992, 2075, 10047, 26657, 2094, 2731, 4520, 1031, 1021, 1033, 3818, 1037, 2047, 4118, 2005, 20177, 1062, 1056, 1998, 10549, 1155, 1056, 1012, 102, 101, 3602, 2008, 1999, 2553, 1997, 2019, 17837, 3279, 3853, 1006, 15262, 12083, 9541, 3367, 1010, 6948, 12083, 9541, 3367, 1007, 2057, 2031, 2019, 3176, 16381, 1156, 1012, 102, 101, 3893, 4973, 2097, 2131, 1156, 2335, 3020, 3635, 2084, 4997, 4973, 1012, 102, 101, 1996, 16381, 1156, 2097, 2036, 5009, 1996, 3635, 2039, 16616, 3627, 2000, 3623, 1996, 3635, 1997, 6270, 4997, 2015, 2062, 24663, 2084, 1997, 6270, 3893, 2015, 1012, 102, 101, 2057, 9375, 1062, 1056, 2004, 1024, 1062, 1056, 1006, 1155, 1056, 1007, 1027, 1049, 1045, 1027, 1015, 1040, 1056, 1006, 1045, 1007, 4654, 2361, 1006, 1597, 1155, 1056, 1156, 1045, 1044, 1056, 1006, 1060, 1045, 1007, 1061, 1045, 1007, 1006, 2184, 1007, 2073, 1156, 1045, 1027, 1015, 1013, 1156, 2065, 1061, 102, 101, 2000, 7163, 28732, 1996, 7561, 2057, 6148, 2000, 7163, 28732, 1062, 1056, 2007, 4847, 2000, 1155, 1056, 1012, 102, 101, 2011, 2635, 1996, 2034, 13819, 1997, 1006, 2184, 1007, 1998, 1041, 16211, 3436, 2009, 2000, 5717, 1998, 10449, 14869, 2860, 1039, 1010, 1052, 1027, 1049, 1045, 1027, 1015, 1040, 1056, 1006, 1045, 1007, 1064, 1061, 1045, 1027, 1039, 1602, 1044, 1006, 1060, 1045, 1007, 1027, 1052, 1010, 2073, 1039, 1010, 1052, 1596, 1063, 102, 101, 4942, 21532, 1061, 1027, 4654, 2361, 1006, 1155, 1056, 1007, 2057, 6855, 1024, 1039, 1015, 1061, 1015, 1597, 1015, 1013, 1156, 1009, 1039, 1016, 1061, 1015, 1009, 2487, 1013, 1156, 1009, 1039, 1017, 1061, 1016, 1009, 1039, 1018, 1027, 1014, 1006, 2340, 1007, 2073, 2278, 1015, 1027, 1597, 1059, 1009, 1009, 1013, 1156, 1010, 102, 101, 1996, 7117, 1997, 8522, 1006, 2340, 1007, 2064, 2022, 2179, 15973, 2135, 1012, 102, 101, 1062, 1056, 1006, 1155, 1056, 1007, 1028, 1014, 12748, 1062, 1056, 1006, 1155, 1056, 1007, 2003, 18309, 1998, 2038, 2069, 2028, 6263, 29625, 2050, 5410, 10744, 1044, 2007, 10124, 1062, 1056, 2003, 4217, 1012, 102, 101, 2023, 2003, 2178, 2126, 1997, 20177, 1155, 1056, 1998, 2048, 2047, 5410, 26262, 2064, 2022, 4663, 1006, 2613, 29625, 2226, 1010, 5860, 29625, 2226, 1007, 1012, 102, 101, 2027, 11234, 2013, 15262, 5092, 14122, 29625, 2213, 2232, 2069, 1999, 1996, 2126, 1062, 1056, 2003, 10174, 1998, 2005, 16246, 26262, 2057, 2275, 1155, 1056, 2000, 2022, 1996, 5576, 1997, 1006, 2340, 1007, 2029, 7163, 28732, 2015, 1062, 1056, 1012, 102, 101, 1996, 2206, 5433, 6235, 1996, 6388, 16437, 1012, 102, 101, 2057, 2036, 6235, 1998, 20302, 23274, 7885, 2864, 2478, 1996, 2176, 12992, 2075, 13792, 1998, 2416, 3793, 4937, 20265, 26910, 5410, 26262, 2008, 2020, 2649, 1999, 1996, 3025, 5433, 1012, 102, 101, 2057, 2864, 17537, 9312, 2006, 1996, 16913, 9331, 2618, 3975, 1997, 1996, 26665, 1011, 17405, 2581, 2620, 2951, 13462, 9227, 2011, 2585, 4572, 1012, 102, 101, 1996, 3975, 3774, 1997, 2260, 1010, 3938, 2475, 5491, 1997, 2029, 1023, 1010, 3438, 2509, 2024, 2109, 2005, 2731, 1998, 1017, 1010, 25926, 2005, 5604, 29625, 10760, 2206, 17463, 3217, 9623, 7741, 2001, 2864, 1024, 2035, 2616, 2020, 4991, 2000, 2896, 2553, 1998, 26136, 6593, 14505, 6017, 1006, 5187, 2620, 1007, 2020, 3718, 1012, 102, 101, 2057, 3718, 2644, 2616, 2013, 1037, 2862, 1997, 4720, 2509, 2394, 2616, 1012, 102, 101, 2057, 2109, 1996, 8716, 7872, 5017, 1031, 1023, 1033, 1998, 6025, 2069, 2216, 3408, 2383, 6254, 6075, 3469, 2084, 1017, 1012, 102, 101, 2044, 1996, 17463, 3217, 9623, 7741, 1996, 13931, 4838, 1020, 1010, 22431, 5664, 2838, 1006, 3408, 1007, 1012, 102, 101, 2057, 2109, 1996, 17917, 1049, 2422, 1031, 1020, 1033, 7375, 1997, 17917, 2213, 2007, 1037, 7399, 16293, 1012, 102, 101, 2241, 2006, 4696, 2946, 2057, 2031, 4217, 1037, 2275, 1997, 2385, 26665, 1011, 17405, 2581, 2620, 7236, 1012, 102, 101, 2070, 1997, 2068, 2024, 2312, 1006, 7796, 1010, 9353, 4160, 1007, 1998, 2070, 2428, 2235, 1006, 14557, 1010, 8899, 1007, 2383, 2069, 1037, 2261, 4973, 29625, 8545, 4738, 12992, 2075, 12441, 2465, 28295, 2000, 2191, 20932, 3251, 1037, 6254, 7460, 2000, 1037, 4696, 2030, 2025, 1012, 102, 101, 2057, 4137, 2035, 5491, 2383, 1996, 4696, 1037, 3893, 2465, 1998, 2035, 2060, 5491, 1037, 4997, 2465, 29625, 8545, 2743, 1037, 2275, 1997, 6036, 7885, 2005, 1037, 2309, 26665, 4696, 2478, 14930, 1997, 2035, 1996, 2649, 12992, 2075, 13792, 1998, 5410, 26262, 1012, 102, 101, 1999, 2035, 7885, 1037, 2193, 1997, 6241, 1997, 4083, 2001, 2275, 2000, 3998, 1012, 102, 101, 2057, 7718, 1996, 14930, 1997, 1996, 2206, 11709, 1024, 1167, 1024, 1014, 29625, 2487, 1010, 1014, 29625, 2475, 1025, 1040, 6053, 1024, 1014, 1010, 2184, 1010, 2753, 1010, 2531, 1025, 1156, 1024, 1016, 1010, 1018, 1010, 1022, 1012, 102, 101, 2005, 2169, 12992, 2075, 9896, 2057, 4653, 1996, 2190, 7551, 2478, 1996, 3115, 2592, 26384, 20069, 3556, 2006, 1996, 3231, 2951, 13462, 1012, 102, 101, 2057, 19148, 2008, 10549, 1996, 2190, 2836, 2006, 1996, 3231, 2275, 19528, 8520, 2037, 3570, 1012, 102, 101, 2021, 1996, 6614, 1997, 2023, 7551, 2001, 2000, 2131, 1996, 2801, 2055, 1996, 2190, 2825, 2836, 1006, 3356, 5391, 1007, 1997, 2536, 13792, 1012, 102, 101, 2795, 1015, 3065, 3463, 2005, 4217, 7236, 1012, 102, 101, 2004, 2057, 2064, 2156, 2013, 1996, 20185, 6948, 12083, 9541, 3367, 1006, 6948, 2226, 1007, 2003, 7444, 1010, 2628, 2011, 1996, 6948, 5092, 14122, 1006, 6948, 1007, 1998, 15262, 12083, 9541, 3367, 1006, 1057, 1007, 1012, 102, 101, 15262, 5092, 14122, 1006, 15262, 1007, 1998, 7399, 17917, 2213, 2024, 2521, 2369, 1012, 102, 101, 2006, 2312, 7236, 15262, 1010, 1057, 1998, 17917, 2213, 2024, 1037, 2210, 2488, 2084, 6948, 1998, 6948, 2226, 1010, 2021, 2004, 2057, 2693, 2000, 3760, 7236, 1996, 11647, 1997, 6948, 2226, 1006, 1998, 2036, 6948, 1998, 1057, 2000, 2070, 6698, 1007, 4025, 2000, 3711, 1012, 102, 101, 15262, 12083, 9541, 3367, 2038, 1996, 3444, 1997, 17837, 3279, 3853, 2029, 7126, 1998, 6948, 5092, 14122, 2038, 1996, 7337, 2000, 2424, 2019, 15502, 5257, 1997, 5410, 1044, 22571, 14573, 23072, 1025, 2119, 2122, 2838, 2024, 4117, 1999, 6948, 12083, 9541, 3367, 29625, 2239, 2235, 7236, 15262, 1998, 1057, 2058, 8873, 2102, 1996, 2731, 2951, 1006, 3275, 1017, 1007, 1012, 102, 101, 15262, 12083, 9541, 3367, 2003, 2062, 13070, 2000, 2058, 8873, 13027, 2084, 15262, 5092, 14122, 1012, 102, 101, 2006, 1996, 2060, 2192, 1996, 2836, 1997, 6948, 1998, 6948, 2226, 2006, 1996, 2731, 2275, 17913, 2007, 16922, 4696, 2946, 1010, 2021, 2006, 1996, 3231, 2275, 2009, 3464, 2012, 2055, 1996, 2168, 2504, 1012, 102, 101, 2057, 2064, 2156, 2008, 6948, 1998, 6948, 2226, 2024, 6020, 2000, 15262, 1998, 17917, 2213, 2096, 1057, 2515, 1037, 2210, 4788, 2084, 6948, 1998, 6948, 2226, 29625, 3366, 8663, 18718, 2057, 2864, 1996, 2168, 2275, 1997, 6036, 7885, 2478, 2358, 8609, 7810, 1019, 10671, 2892, 27354, 1012, 102, 101, 2241, 2006, 2779, 20069, 3556, 2058, 1019, 7012, 1010, 2057, 2031, 4217, 2190, 16381, 9563, 2005, 2169, 9896, 1012, 102, 101, 2795, 1016, 3065, 1996, 2779, 20069, 2006, 3231, 2275, 29625, 8545, 2064, 2156, 2008, 3463, 4663, 2011, 2892, 27354, 1006, 2795, 1016, 1007, 2024, 2025, 2521, 2013, 1996, 15502, 1006, 2795, 1015, 1007, 1012, 102, 101, 2005, 13792, 2007, 1037, 2235, 2275, 1997, 11709, 1006, 15262, 5092, 14122, 1010, 17917, 2213, 1007, 1996, 4489, 2090, 15502, 1998, 2892, 27354, 2836, 2003, 2235, 1012, 102, 101, 2045, 2003, 1037, 10889, 2312, 6578, 2005, 1057, 1998, 2130, 3469, 2005, 6948, 29625, 2015, 2615, 2213, 10438, 2190, 1997, 2035, 13792, 2006, 7236, 2007, 2062, 2084, 1015, 1003, 1997, 3893, 2731, 4973, 1010, 2021, 2004, 2057, 9885, 1996, 4696, 2946, 2009, 2003, 2053, 2062, 6975, 1012, 102, 101, 6948, 12083, 9541, 3367, 10438, 2190, 1998, 2025, 2521, 2013, 15502, 1012, 102, 101, 2057, 2036, 5159, 2008, 15502, 16381, 10906, 2024, 2367, 2013, 2216, 4663, 2011, 2892, 27354, 1012, 102, 101, 2836, 1997, 6948, 12083, 9541, 3367, 2003, 3243, 6540, 2006, 1996, 2878, 2846, 1997, 7236, 1997, 2536, 10826, 1012, 102, 101, 3275, 1018, 3065, 5171, 4083, 10543, 1997, 12992, 2075, 13792, 1012, 102, 101, 1999, 1996, 2327, 5216, 2057, 2156, 1996, 5171, 2836, 1997, 15262, 5092, 14122, 2006, 2312, 7236, 1998, 1996, 9808, 6895, 20382, 2015, 2057, 4384, 1999, 15262, 12083, 9541, 3367, 1012, 102, 101, 1996, 3953, 5216, 3065, 6948, 1998, 6948, 2226, 1012, 102, 101, 2057, 11949, 3469, 14523, 1999, 2836, 2084, 2005, 6013, 2007, 15262, 2030, 1057, 29625, 4050, 2012, 1996, 2220, 6241, 1997, 4083, 6948, 1006, 6948, 2226, 1007, 2003, 2025, 6540, 1010, 2021, 2043, 2057, 2693, 2830, 2836, 4152, 2062, 6540, 1010, 2043, 2633, 1996, 9896, 28314, 2015, 1012, 102, 101, 6948, 1998, 6948, 2226, 2224, 2116, 8491, 5410, 1044, 22571, 14573, 23072, 2084, 15262, 2030, 1057, 1012, 102, 101, 2057, 4738, 15262, 1998, 1057, 2005, 3998, 6241, 1006, 3998, 5410, 1044, 22571, 14573, 23072, 2020, 4217, 1007, 1012, 102, 101, 6948, 1998, 6948, 2226, 28314, 2094, 1999, 2105, 2753, 6241, 2005, 2312, 1998, 2105, 1019, 2030, 2625, 6241, 1997, 4083, 2005, 2235, 7236, 1012, 102, 101, 2005, 2070, 1997, 1996, 10479, 7236, 6948, 2226, 3856, 2074, 1015, 2030, 1016, 5410, 1044, 22571, 14573, 23072, 1998, 2081, 2053, 7561, 1011, 2005, 4696, 8899, 2009, 2003, 7182, 2000, 4638, 2005, 1996, 3739, 1997, 1037, 2773, 8899, 1999, 2344, 2000, 2191, 3819, 20932, 29625, 8663, 7363, 4892, 4217, 5410, 26262, 2057, 2156, 2613, 29625, 2213, 2232, 3957, 2190, 2836, 2005, 102, 101, 2023, 2003, 1999, 10388, 2007, 1996, 3463, 2988, 1999, 1031, 2260, 1033, 1012, 102, 101, 2613, 29625, 2213, 2232, 2003, 2628, 2011, 5860, 29625, 2213, 2232, 1998, 2613, 29625, 14277, 1012, 102, 101, 2569, 5410, 26262, 2106, 2025, 5335, 1996, 2836, 1011, 2025, 2130, 1999, 5257, 2007, 1996, 12992, 2075, 9896, 2027, 2020, 2881, 2000, 2147, 2362, 1012, 102, 101, 2057, 2165, 1016, 2922, 7236, 1024, 7796, 1998, 9353, 4160, 1012, 102, 101, 2731, 2275, 3774, 1997, 2035, 4997, 1998, 1037, 2193, 1997, 18154, 3479, 3893, 2731, 4973, 1012, 102, 101, 2011, 17739, 1037, 2193, 1997, 3893, 2731, 4973, 2057, 7976, 2135, 2580, 1037, 2235, 4696, 29625, 8545, 2864, 1996, 2168, 2275, 1997, 6036, 7885, 2004, 1999, 2930, 1018, 29625, 2475, 1012, 102, 101, 2005, 2169, 12992, 2075, 9896, 2057, 4653, 1996, 2190, 7551, 2478, 1996, 20069, 3556, 2006, 1996, 3231, 2275, 1012, 102, 101, 2023, 2965, 2057, 2265, 1996, 3356, 5391, 1997, 1996, 9896, 1012, 102, 101, 3275, 1019, 3065, 1996, 3463, 2029, 2024, 11341, 1012, 102, 101, 3025, 7885, 3662, 2008, 6948, 2226, 2003, 2190, 2006, 17837, 2731, 4520, 1010, 2138, 2009, 2515, 2025, 2058, 8873, 2102, 1998, 11214, 1037, 12949, 2235, 2193, 1997, 5410, 26262, 1012, 102, 101, 2021, 3275, 1019, 3065, 1037, 2367, 3861, 1012, 102, 101, 2004, 1996, 2193, 1997, 3893, 2731, 4973, 17913, 1010, 1996, 2836, 1997, 6948, 2226, 1006, 6948, 1007, 2036, 12099, 9885, 1010, 2021, 1996, 2836, 1997, 1057, 1006, 15262, 1007, 3464, 2471, 2012, 1996, 2168, 2504, 29625, 8545, 5159, 2471, 1996, 2168, 2477, 2007, 9353, 4160, 1024, 1057, 2003, 2145, 1996, 2190, 1010, 4876, 2628, 102, 101, 2836, 1997, 15262, 1998, 6948, 2003, 3532, 1998, 2044, 1996, 2193, 1997, 3893, 2731, 4973, 9010, 4330, 5004, 2753, 1010, 20069, 2003, 2625, 2084, 1014, 29625, 2487, 1012, 102, 101, 2144, 7551, 3662, 2008, 1996, 2190, 2825, 2836, 1997, 6948, 1998, 6948, 2226, 2024, 2521, 2369, 2013, 15262, 1998, 1057, 1010, 2057, 2106, 1050, 29618, 2102, 2448, 2358, 8609, 7810, 2892, 27354, 29625, 14277, 2226, 1006, 6948, 1007, 2864, 2200, 2092, 2006, 8100, 17837, 2951, 13462, 2015, 1012, 102, 101, 2021, 2043, 2057, 7976, 2135, 3443, 2019, 2235, 4696, 1010, 1996, 2836, 1997, 6948, 2226, 10548, 12099, 29625, 15222, 2015, 6083, 2008, 2045, 2003, 1037, 8050, 4489, 2090, 8100, 2235, 1998, 7976, 2135, 2235, 7236, 1012, 102, 101, 2057, 2228, 2008, 26665, 1005, 10195, 4937, 20265, 21885, 2075, 5491, 2081, 7796, 2200, 7578, 1006, 5041, 1998, 2025, 3563, 1007, 1010, 2096, 1037, 2235, 4696, 2066, 8899, 2003, 2200, 3563, 1012, 102, 101, 3602, 2008, 1996, 3231, 2275, 2003, 15704, 2061, 2009, 12950, 2434, 4696, 1006, 2038, 1996, 2168, 4353, 2004, 2434, 4696, 1007, 1012, 102, 101, 2000, 2191, 2204, 20932, 2478, 2235, 2731, 1006, 1998, 2312, 7578, 5604, 1007, 7796, 2028, 2038, 2000, 1000, 2058, 8873, 2102, 1000, 2011, 2635, 2035, 1006, 2025, 9352, 3278, 1007, 2838, 1997, 1996, 2731, 2951, 1012, 102, 101, 2006, 1996, 2060, 2192, 2057, 2031, 2000, 2022, 2200, 6176, 1998, 2202, 2069, 2428, 3278, 2838, 2000, 2191, 2204, 20932, 2006, 8899, 1012, 102, 101, 2023, 3259, 13999, 6948, 12083, 9541, 3367, 12992, 2075, 9896, 1012, 102, 101, 2057, 3073, 2119, 9373, 1998, 17537, 3350, 2008, 6948, 12083, 9541, 3367, 2003, 2092, 10897, 2005, 3793, 4937, 20265, 26910, 2005, 17837, 2951, 4520, 29625, 14277, 12083, 9541, 3367, 2038, 2116, 6666, 2058, 17978, 1011, 2241, 8107, 1024, 10713, 18287, 2012, 16452, 15502, 5576, 1010, 2092, 1011, 4225, 19143, 9181, 1010, 16655, 26426, 3279, 3853, 1998, 2224, 8491, 5410, 1044, 22571, 14573, 23072, 1012, 102], "tgt": [1, 24582, 19968, 2494, 1015, 7399, 4730, 12992, 2075, 2005, 17837, 2951, 13462, 2015, 18414, 3089, 3501, 4649, 7724, 8586, 1010, 3533, 2546, 8852, 2820, 1010, 10307, 2198, 8233, 2063, 1011, 4202, 1010, 2548, 20977, 2118, 1997, 2414, 1010, 2866, 24582, 19968, 2494, 1016, 14354, 2045, 2024, 5385, 2454, 1997, 13481, 1998, 1016, 2454, 1997, 2068, 2024, 16583, 2015, 2215, 2000, 3857, 1037, 2465, 18095, 2000, 10782, 16583, 2015, 2013, 1996, 2717, 1997, 13481, 1037, 3151, 11499, 2465, 18095, 1006, 1041, 29625, 2290, 29625, 3761, 1007, 2052, 2025, 2130, 5060, 10307, 2004, 2019, 9178, 2057, 2079, 23961, 2215, 2008, 999, 3, 24582, 19968, 2494, 1017, 3291, 4292, 4895, 26657, 2094, 2951, 13462, 1016, 4280, 1024, 3893, 1006, 2235, 1007, 4997, 1006, 2312, 1007, 3345, 1037, 12441, 2465, 18095, 2000, 3584, 3811, 4895, 26657, 2094, 4280, 24582, 19968, 2494, 1018, 2256, 5576, 7705, 2057, 2097, 2224, 12992, 2075, 11506, 2116, 3722, 1998, 24949, 4937, 20265, 26910, 3513, 1006, 5410, 26262, 1007, 2046, 1037, 2309, 3811, 8321, 4937, 20265, 26910, 3627, 1996, 3722, 3513, 2024, 4738, 25582, 2135, 1025, 2169, 3627, 2003, 4738, 2006, 4973, 2029, 2024, 2087, 3697, 2000, 26268, 2011, 11003, 3513, 24582, 19968, 2494, 1019, 12685, 12992, 2075, 13792, 5410, 26262, 6388, 16437, 3463, 15306, 24582, 19968, 2494, 1020, 3141, 8107, 1024, 15262, 5092, 14122, 2445, 2731, 4973, 1006, 1060, 2487, 1010, 1061, 2487, 1007, 1010, 1006, 1060, 2213, 1010, 1061, 2213, 1007, 3988, 4697, 1040, 2692, 1006, 1045, 1007, 1027, 1015, 1013, 1049, 12316, 1063, 1009, 2487, 1010, 1011, 2487, 1065, 2005, 1056, 1027, 1015, 2102, 3413, 4353, 26718, 2000, 5410, 4553, 2121, 2131, 5410, 10744, 1044, 2102, 1024, 1060, 1054, 5454, 1056, 1006, 2241, 2006, 2836, 1997, 1044, 2102, 1007, 10651, 26718, 1009, 2487, 1006, 1045, 1007, 1027, 26718, 1006, 1045, 1007, 4654, 2361, 1006, 1011, 1056, 12316, 1044, 2102, 1006, 8418, 1007, 1007, 1013, 1062, 2102, 2345, 10744, 1024, 1042, 1006, 1060, 1007, 1027, 1056, 1056, 1044, 2102, 1006, 1060, 1007, 24582, 19968, 2494, 1021, 15262, 5092, 14122, 1011, 26406, 5410, 10744, 1044, 1006, 1060, 1007, 3696, 1997, 1044, 1006, 1060, 1007, 2003, 1996, 10173, 12441, 3830, 10194, 1064, 1044, 1006, 1060, 1007, 1064, 2004, 1037, 7023, 1056, 7711, 1996, 3747, 1997, 2169, 1044, 2102, 1006, 1060, 1007, 24582, 19968, 2494, 1022, 2062, 12992, 2075, 13792, 13792, 11234, 1999, 1996, 2126, 1997, 3988, 6026, 15871, 1040, 2692, 1006, 1045, 1007, 1006, 28616, 26266, 9031, 5366, 1007, 1998, 2039, 16616, 2068, 1018, 12992, 2075, 13792, 1024, 15262, 5092, 14122, 20505, 3921, 1057, 5092, 14122, 17837, 3279, 3853, 1009, 20505, 6948, 5092, 14122, 7399, 4730, 1006, 15502, 5576, 1007, 6948, 12083, 9541, 3367, 2256, 3818, 5576, 1006, 6948, 1009, 17837, 1007, 24582, 19968, 2494, 1023, 2445, 2731, 4973, 1006, 1060, 2487, 1010, 1061, 2487, 1007, 1010, 1006, 1060, 2213, 1010, 1061, 2213, 1007, 3988, 4697, 1040, 2692, 1006, 1045, 1007, 1027, 1015, 1013, 1049, 12316, 1063, 1009, 2487, 1010, 1011, 2487, 1065, 2005, 1056, 1027, 1015, 2102, 3413, 4353, 26718, 2000, 5410, 4553, 2121, 2131, 5410, 10744, 1044, 2102, 1024, 1060, 1054, 5454, 1056, 10651, 26718, 1009, 2487, 1006, 1045, 1007, 1027, 26718, 1006, 1045, 1007, 4654, 2361, 1006, 1011, 1056, 12316, 1044, 2102, 1006, 8418, 1007, 1007, 1013, 1062, 2102, 2345, 10744, 1024, 1042, 1006, 1060, 1007, 1027, 1056, 1056, 1044, 2102, 1006, 1060, 1007, 12992, 2075, 9896, 5966, 12992, 2075, 13792, 11234, 1999, 2122, 1016, 3210, 24582, 19968, 2494, 2184, 1057, 5092, 14122, 1011, 17837, 3279, 3853, 2275, 1024, 1040, 2692, 1006, 1045, 1007, 2061, 2008, 1040, 2692, 1006, 3893, 1007, 1013, 1040, 2692, 1006, 4997, 1007, 1027, 10651, 26718, 1009, 2487, 1006, 1045, 1007, 1024, 3623, 3635, 1997, 6270, 4997, 2015, 2062, 2084, 2006, 6270, 3893, 2015, 9885, 3635, 1997, 2995, 3893, 2015, 2625, 2084, 2006, 2995, 4997, 2015, 3893, 4973, 5441, 3020, 3635, 1006, 28616, 26266, 9031, 3465, 1007, 24582, 19968, 2494, 2340, 6948, 5092, 14122, 7399, 4730, 2275, 1024, 1040, 2692, 1006, 1045, 1007, 1027, 1015, 1013, 1049, 10651, 26718, 1009, 2487, 1024, 9611, 6948, 1024, 12098, 21693, 2378, 6948, 20915, 2050, 1010, 1055, 29625, 2102, 29625, 1045, 1006, 1040, 1006, 1045, 1007, 12316, 22563, 1006, 8418, 1007, 1007, 6948, 20915, 2050, 1025, 1047, 1027, 1015, 2102, 2073, 1015, 1013, 1037, 1026, 1040, 1006, 1045, 1007, 1026, 1015, 1013, 1038, 2275, 2000, 2474, 17643, 3070, 2937, 4800, 24759, 10136, 2065, 1045, 1040, 1006, 1045, 1007, 12316, 1044, 2102, 1006, 8418, 1007, 1026, 6948, 20915, 2050, 1010, 15502, 5576, 24582, 19968, 2494, 2260, 6948, 5092, 14122, 26406, 12098, 21693, 2378, 6948, 20915, 2050, 1055, 29625, 2102, 29625, 1045, 1006, 1040, 1006, 1045, 1007, 12316, 22563, 1006, 8418, 1007, 1007, 6948, 20915, 2050, 1047, 1027, 1015, 2133, 1056, 2073, 1015, 1013, 1037, 1026, 1040, 1006, 1045, 1007, 1026, 1015, 1013, 1038, 1040, 1006, 1015, 1007, 1040, 1006, 1016, 1007, 1040, 1006, 1017, 1007, 1040, 1006, 1049, 1007, 1044, 2487, 1009, 1011, 1009, 1044, 2475, 1011, 1011, 1009, 1009, 6948, 20915, 2050, 1044, 2102, 1009, 1011, 1009, 1009, 2731, 2742, 15871, 5410, 26262, 24582, 19968, 2494, 2410, 6948, 5092, 14122, 2742, 1040, 1006, 1015, 1007, 1040, 1006, 1016, 1007, 1040, 1006, 1017, 1007, 1044, 2487, 1009, 1014, 29625, 2509, 1040, 1006, 1015, 1007, 1009, 1014, 29625, 2581, 1040, 1006, 1016, 1007, 1011, 1014, 29625, 2475, 1040, 1006, 1017, 1007, 6948, 20915, 2050, 1044, 2475, 1009, 1014, 29625, 2487, 1040, 1006, 1015, 1007, 1011, 1014, 29625, 2549, 1040, 1006, 1016, 1007, 1011, 1014, 29625, 2629, 1040, 1006, 1017, 1007, 6948, 20915, 2050, 1044, 2509, 1009, 1014, 29625, 2629, 1040, 1006, 1015, 1007, 1011, 1014, 29625, 2487, 1040, 1006, 1016, 1007, 1011, 1014, 29625, 2509, 1040, 1006, 1017, 1007, 6948, 20915, 2050, 2731, 2742, 15871, 12098, 21693, 2378, 6948, 20915, 2050, 1055, 29625, 2102, 29625, 1045, 1006, 12316, 22563, 1006, 8418, 1007, 1040, 1006, 1045, 1007, 1007, 6948, 20915, 2050, 1047, 1027, 1015, 2133, 1017, 2073, 1015, 1013, 1037, 1026, 1040, 1006, 1045, 1007, 1026, 1015, 1013, 1038, 7023, 2378, 27108, 2890, 6593, 2135, 6219, 11178, 6219, 5410, 26262, 24582, 19968, 2494, 2403, 6948, 12083, 9541, 3367, 1011, 17837, 3279, 1009, 6948, 2275, 1024, 1040, 2692, 1006, 1045, 1007, 2061, 2008, 1040, 2692, 1006, 3893, 1007, 1013, 1040, 2692, 1006, 4997, 1007, 1027, 10651, 26718, 1009, 2487, 1024, 9611, 6948, 1010, 18478, 6948, 20915, 2050, 2021, 2275, 2367, 28616, 26266, 9031, 3465, 19202, 2005, 1040, 1006, 1045, 1007, 1006, 2335, 3020, 2005, 3893, 4973, 1007, 1996, 2717, 2004, 1999, 6948, 5092, 14122, 3602, 1024, 2003, 7953, 16381, 1012, 3, 6948, 20915, 2050, 2003, 7399, 4730, 20600, 8023, 24582, 19968, 2494, 2321, 12654, 1997, 12992, 2075, 13792, 17837, 3279, 3853, 28314, 2015, 2000, 3795, 23569, 28591, 15262, 5092, 14122, 1057, 5092, 14122, 6948, 5092, 14122, 6948, 12083, 9541, 3367, 24582, 19968, 2494, 2385, 5410, 26262, 2028, 1011, 2504, 3247, 3392, 1006, 2065, 1011, 2059, 3627, 1007, 1024, 2065, 2773, 1059, 5158, 1999, 1037, 6254, 1060, 2709, 1052, 2842, 2709, 1050, 1052, 1998, 1050, 2024, 2613, 3616, 4217, 2241, 2006, 28616, 26266, 9031, 3465, 15871, 26718, 1006, 1045, 1007, 17841, 1996, 3696, 1997, 1052, 1998, 1050, 2004, 1996, 10173, 12441, 3830, 10194, 1064, 1052, 1064, 1998, 1064, 1050, 1064, 2004, 1996, 7023, 24582, 19968, 2494, 2459, 6388, 16437, 26665, 2739, 20357, 4790, 1006, 26665, 9131, 1027, 1056, 2361, 1013, 1006, 1056, 2361, 1009, 1042, 2078, 1007, 20069, 1027, 1016, 28139, 2278, 28667, 1013, 1006, 3653, 2278, 1009, 28667, 1007, 24582, 19968, 2494, 2324, 5171, 8146, 12042, 2731, 2951, 13462, 2035, 4083, 13792, 2265, 2714, 2836, 4895, 26657, 2094, 2731, 2951, 13462, 15262, 5092, 14122, 2058, 8873, 3215, 6948, 12083, 9541, 3367, 2515, 2025, 2058, 8873, 2102, 28314, 2015, 3435, 2478, 2069, 1037, 2261, 5410, 26262, 1057, 5092, 14122, 1998, 6948, 5092, 14122, 2024, 4873, 1999, 2090, 24582, 19968, 2494, 2539, 12042, 2951, 13462, 5171, 5248, 24582, 19968, 2494, 2322, 4895, 26657, 2094, 2951, 13462, 15262, 5092, 14122, 2058, 8873, 3215, 24582, 19968, 2494, 2538, 4895, 26657, 2094, 2951, 13462, 6948, 12083, 9541, 3367, 2261, 27758, 2015, 1006, 2184, 1007, 2644, 2044, 2053, 7218, 3444, 2003, 2187, 24582, 19968, 2494, 2570, 26665, 7236, 20069, 2006, 3231, 2275, 2130, 17837, 4696, 1006, 2946, 1007, 15262, 1057, 6948, 6948, 2226, 17917, 2213, 7796, 1006, 23090, 2581, 1007, 1014, 29625, 2683, 2581, 1014, 29625, 2683, 2581, 1014, 29625, 2683, 2581, 1014, 29625, 2683, 2487, 1014, 29625, 2683, 2620, 9353, 4160, 1006, 21875, 1007, 1014, 29625, 2683, 2487, 1014, 29625, 2683, 2549, 1014, 29625, 2620, 2620, 1014, 29625, 2620, 2549, 1014, 29625, 2683, 2549, 2769, 1011, 23292, 1006, 5187, 2620, 1007, 1014, 29625, 26187, 1014, 29625, 19841, 1014, 29625, 2575, 2509, 1014, 29625, 26187, 1014, 29625, 2581, 2575, 3037, 1006, 4090, 2581, 1007, 1014, 29625, 26187, 1014, 29625, 2575, 2683, 1014, 29625, 28154, 1014, 29625, 28756, 1014, 29625, 26187, 9781, 1006, 18596, 1007, 1014, 29625, 2620, 2487, 1014, 29625, 2620, 2581, 1014, 29625, 2620, 2475, 1014, 29625, 2620, 2509, 1014, 29625, 17914, 1043, 16275, 1006, 7886, 1007, 1014, 29625, 2581, 2620, 1014, 29625, 17914, 1014, 29625, 21084, 1014, 29625, 28756, 1014, 29625, 2620, 2487, 2482, 15671, 2015, 1006, 2753, 1007, 1014, 29625, 26224, 1014, 29625, 26187, 1014, 29625, 2575, 2509, 1014, 29625, 26187, 1014, 29625, 25746, 6557, 1006, 4464, 1007, 1014, 29625, 2575, 2620, 1014, 29625, 2620, 2683, 1014, 29625, 2683, 2629, 1014, 29625, 2683, 2629, 1014, 29625, 2575, 2620, 7954, 1011, 5438, 1006, 2382, 1007, 1014, 29625, 28154, 1014, 29625, 2581, 2581, 1014, 29625, 26187, 1014, 29625, 2620, 2487, 1014, 29625, 19961, 9004, 1011, 18178, 2213, 1006, 2322, 1007, 1014, 29625, 2692, 2509, 1014, 29625, 16048, 1014, 29625, 2692, 2509, 1014, 29625, 16147, 1014, 29625, 16576, 2599, 1006, 2321, 1007, 1014, 29625, 11387, 1014, 29625, 2575, 2581, 1014, 29625, 18827, 1014, 29625, 19961, 1014, 25176, 1011, 7954, 1006, 2410, 1007, 1014, 29625, 14142, 1014, 29625, 2581, 2509, 1014, 29625, 19481, 1014, 29625, 22025, 1014, 29625, 17465, 2598, 24072, 1006, 1019, 1007, 1014, 1014, 1014, 29625, 19317, 1014, 29625, 23352, 1014, 8899, 1006, 1019, 1007, 1014, 1014, 1014, 29625, 11387, 1015, 29625, 8889, 1014, 29625, 16703, 14557, 1006, 1017, 1007, 1014, 29625, 22275, 1014, 29625, 22275, 1014, 29625, 24594, 1014, 29625, 20842, 1014, 29625, 16068, 18996, 11039, 3270, 1006, 1016, 1007, 1014, 1014, 1014, 29625, 11387, 1014, 29625, 2620, 2683, 1014, 2779, 1014, 29625, 22610, 1014, 29625, 28154, 1014, 29625, 25746, 1014, 29625, 2581, 2475, 1014, 29625, 21472, 24582, 19968, 2494, 2603, 6948, 12083, 9541, 3367, 5443, 29625, 1057, 5092, 14122, 24582, 19968, 2494, 2484, 2087, 2590, 2838, 1006, 27674, 2616, 1007, 7796, 1006, 23090, 2581, 1007, 2753, 1024, 14931, 1010, 5658, 1010, 5618, 1010, 11443, 4859, 1010, 14021, 2099, 3037, 1006, 4090, 2581, 1007, 3963, 1024, 3446, 1010, 2924, 1010, 2194, 1010, 2095, 1010, 7473, 2102, 2482, 15671, 2015, 1006, 2753, 1007, 2382, 1024, 12486, 1010, 15960, 1010, 6240, 1010, 7922, 1010, 3190, 25176, 1011, 7954, 1006, 2410, 1007, 1017, 1024, 7954, 1010, 25176, 4168, 2389, 1010, 25176, 4783, 2319, 2598, 24072, 1006, 1019, 1007, 1016, 1024, 21443, 1010, 6557, 1006, 20069, 1027, 1014, 29625, 23352, 1007, 8899, 1006, 1019, 1007, 1015, 1024, 8899, 1006, 20069, 1027, 1015, 29625, 2692, 1007, 14557, 1006, 1017, 1007, 1015, 1024, 14557, 1006, 20069, 1027, 1014, 29625, 20842, 1007, 4696, 2946, 6948, 2226, 2944, 2946, 1006, 2193, 1997, 2838, 1013, 2616, 1007, 24582, 19968, 2494, 2423, 15078, 8122, 15262, 5092, 14122, 1998, 1057, 5092, 14122, 2024, 1996, 7915, 1996, 21304, 6948, 5092, 14122, 1998, 6948, 12083, 9541, 3367, 2024, 1037, 2210, 12430, 6948, 22334, 3138, 2172, 1997, 1996, 2051, 2021, 2144, 6948, 12083, 9541, 3367, 15867, 8491, 5410, 1044, 22571, 14573, 23072, 1996, 2335, 2131, 12435, 2000, 2216, 1997, 15262, 5092, 14122, 24582, 19968, 2494, 2656, 15306, 6948, 12083, 9541, 3367, 2003, 7218, 2005, 3793, 4937, 20265, 26910, 2005, 3811, 4895, 26657, 2094, 2951, 13462, 2015, 2035, 6666, 1006, 2092, 1011, 4225, 7458, 9181, 1010, 16655, 26426, 3279, 3853, 1007, 2265, 2039, 2053, 2058, 8873, 13027, 1024, 2009, 2003, 2583, 2000, 2424, 3722, 1006, 2235, 1007, 1998, 8552, 1006, 2312, 1007, 1044, 22571, 14573, 23072, 2], "src_sent_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1], "segs": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "clss": [0, 19, 50, 86, 105, 136, 169, 197, 256, 308, 368, 384, 438, 449, 458, 479, 510, 530, 544, 567, 624, 640, 671, 704, 759, 787, 817, 839, 896, 919, 941, 970, 994, 1011, 1031, 1056, 1076, 1111, 1177, 1232, 1278, 1320, 1370, 1410, 1464, 1519, 1574, 1631, 1643, 1696, 1754, 1772, 1793, 1836, 1899, 1950, 1967, 1988, 2005, 2034, 2090, 2111, 2130, 2165, 2190, 2208, 2224, 2278, 2311, 2342, 2362, 2419, 2444, 2462, 2504, 2557, 2583, 2611, 2670, 2721, 2756, 2791, 2820, 2874, 2928, 2948, 3009, 3039, 3052, 3085, 3122, 3151, 3165, 3194, 3247, 3267, 3322, 3378, 3393, 3429, 3457, 3499, 3509, 3541, 3567, 3623, 3638, 3661, 3681, 3701, 3721, 3769, 3814, 3830, 3870, 3895, 3913, 3939, 3949, 3987, 4003, 4050, 4112, 4130, 4166, 4215, 4236, 4272, 4301, 4350, 4364, 4382, 4403, 4416, 4448, 4459, 4508, 4527, 4548, 4574, 4637, 4652, 4670, 4697, 4710, 4728, 4761, 4782, 4795, 4806, 4838, 4848, 4904, 4934, 4984, 5022, 5056, 5080, 5118, 5143, 5156], "src_txt": ["the paper extends the notion of linear programming boosting to handle uneven datasets .", "extensive experiments with text classification problem compare the performance of a number of different boosting strategies , concentrating on the problems posed by uneven datasets .", "boosting is a method of combining so - called weak learners that individually only perform slightly better than a random classifier into a weighted combination that classifies with high accuracy .", "in general boosting has been shown to exhibit a remarkable resistance to overfitting .", "an explanation for this phenomenon suggests that this results from boosting optimising the margin of the underlying weighted combination of weak learners [ 10 ] .", "this interpretation has suggested a number of modifications of the underlying boosting strategy through changing the measure of the margin distribution that is optimised [ 8 ] .", "taking a 1 - norm of the slack variables and optimising the 1 - norm of the coefficients leads to a linear programme .", "if this is solved by so - called column generation , the resulting algorithm can be seen as a boosting algorithm [ 2,3 ] , where the primal solution gives the weightings of the weak learners and the dual solution the distributions over examples.the question of how to adapt boosting to handle uneven training sets was considered by karakoulas and shawe - taylor [ 7 ] .", "they introduced the u-boost algorithm that biased the distribution weighting in favour of the examples from the smaller class.the aim of the current paper is to introduce a version of the linear programming boosting that is tuned for uneven datasets .", "this is a natural approach to handling uneven datasets as the algorithm optimises a cost criterion that can be adapted to reflect the unevenness of the dataset.at the same time we present extensive experiments comparing a number of different boosting strategies and methods of weak learner generation .", "our main test bed for the experimental work is the reuters document collection .", "in this section we introduce the boosting algorithms considered in our experiments , including the adaptation of the linear programming algorithm for uneven datasets.boosting is a method to find a highly accurate classification rule by combining many weak or base hypotheses .", "each weak hypothesis may be only moderately accurate .", "weak learners are trained sequentially .", "each weak learner is trained on examples which the preceding weak learners found most difficult to classify .", "let s = ( x 1 , y 1 ) , . . . , ( x m , y m ) be a set of training examples .", "each instance x i belongs to a domain x and has assigned a single class y i .", "each class y i belongs to a finite label space y .", "in this paper we focus on binary classification problems in which y = { \u2212 1 , +1 } .", "we call examples having y i = +1 positive and those having y i = \u2212 1 negative examples.a weak learning algorithm accepts as input a sequence of training examples s and a distribution d t ( where d t ( i ) could be interpreted as the misclassification cost of the i - th training example ) .", "based on this input the weak learner outputs a weak hypothesis h .", "we interpret the sign of h ( x ) as the predicted label and the magnitude | h ( x ) | as the confidence in that prediction .", "the parameter \u03b1 t is chosen to measure the importance proceedings of the twentieth international conference on machine learning ( icml - 2003 ) , washington dc , 2003 .", "given \u03b2 and a training set : s = ( x 1 , y 1 ) , . . . , ( x m , y m ) wherex i \u2208 x , y i \u2208 { \u2212 1 , +1 } initialise : d 1 ( i ) = w + if y i = +1 w \u2212 else where w + w \u2212 = \u03b2 and m i = 1 d 1 ( i ) = 1 for t = 1 , . . . , t : pass distribution d t to weak learner get weak hypothesis h t : x \u2192 r choose \u03b1 t \u2208 r update : d t +1 ( i ) = dt ( i ) exp ( \u2212 \u03b1t\u03b2iyiht ( xi ) ) zt where \u03b2 i = 1 \u03b2 if y i = +1 and 1 if otherwise , and z t is a normalization factor output the final hypothesis : of the weak hypothesis h t in the final linear combination of weak hypotheses.f ( x ) = t t = 1 \u03b1 t h t ( x ) at each round of boosting adaboost [ 11 ] increases the weights of wrongly classified training examples ( i.e. if the signs of h t and y i differ ) and decreases the weights of correctly classified examples .", "a common scenario when learning on imbalanced data sets is that the trained classifier classifies all examples to the major class .", "in text classification we often encounter this problem when we are learning a binary classifier to separate a small topic from the rest of the documents .", "in order to avoid this problem , we would like to emphasise the importance of the smaller class .", "we assume that the smaller class is the positive class while the dominant class is the negative one.adauboost [ 7 ] utilises most of the ideas of adaboost [ 11 ] , but introduces an unequal loss function and modified weight updating rule .", "the choice of a parameter \u03b2 will force uneven misclassification costs for training examples ( figure 1 ) .", "examples from the smaller , positive , class will initially get assigned \u03b2 times larger weights than negative examples .", "the parameter \u03b2 will also guide the weight updating rule to increase the weight of false negatives more aggressively than of false positives .", "on the other hand it will decrease the weights of true positives more conservatively than of true negatives .", "under this updating rule the weights of positive examples typically maintain higher values .", "each weak hypothesis therefore tends to correctly classify more positive examples , since they maintain higher weights .", "the final hypothesis , a linear combination of the weak hypotheses , will also correctly classify more positive examples .", "lpboost [ 5 ] is a linear program ( lp ) approach to boosting .", "the paper [ 5 ] shows that taking a 1 - norm of the slack variables and optimising the 1 - norm of the coefficients leads to a linear programme .", "if this is solved by so - called column generation , the resulting algorithm can be seen as a boosting algorithm [ 2,3 ] , where the primal solutions give the weightings of the weak learners and the dual solutions the distributions over examples.lpboost iteratively optimises dual misclassification costs and dynamically generates weak hypotheses to make new lp columns .", "in contrast to gradient boosting algorithms , which may only converge in the limit , lpboost converges in a finite number of iterations to a globally optimal solution satisfying well - defined optimality conditions.one would expect lpboost to be computationally expensive .", "we found , however , that an iteration of lpboost is slightly more expensive than an iteration of adaboost , but on the other hand lpboost needs far fewer iterations than adaboost to converge .", "the lpboost algorithm [ 5 ] is motivated from a generalisation analysis that bounds the test error in terms of the 1 - norm of the vector of coefficients , margin achieved and the slack variables .", "the minimisation of the bound leads to a linear programme that gives the formulation above when converted to the dual.we now give a similar motivation for the uneven version of the lpboost algorithm , we have named lpuboost .", "in this case we assume that the cost of misclassifying a positive example ( assuming that the positive class is the less populated ) is larger than that of misclassifying a negative example .", "hence , the loss l f associated with a classification function f is given byl f ( x , y ) = \u03b2p ( f ( x ) \u2264 0 and y = 1 ) + p ( f ( x ) \u2265 0 and y = \u2212 1 ) , where as with adauboost \u03b2 > 1 gives the higher cost of misclassification of positive examples .", "we can upper bound this loss as followsl f ( x , y ) \u2264 \u03b2 ( 1 + y ) 2\u03c1 ( \u03c1 \u2212 f ( x ) ) + + 1 \u2212 y 2\u03c1 ( f ( x ) + \u03c1 ) + , ( 1 ) where \u03c1 > 0 and ( \u00b7 ) + denotes the function that is the identity if its argument is greater than 0 and 0 otherwise .", "we can now apply the rademacher technique [ 1 ] given \u03b2 and a training set : s = ( x 1 , y 1 ) , . . . , ( x m , y m ) where to bound the loss in terms of its empirical value and the rademacher complexity of the function class .", "the rademacher complexity of a class does not increase if we move to its convex hull and so the bound involves the 1 - norm of the slack variables scaled by the inverse margin plus the rademacher complexity of the weak learners again scaled by the inverse margin .", "we omit the details because of space constraints .", "the resulting bound is optimised by the following linear programme : for lpuboost we take the same lp formulation of boosting as in lpboost , but we introduce a new parameter \u03b2 having the same role as in adauboost .", "positive examples have a \u03b2 times higher bound on their misclassification costsx i \u2208 x , y i \u2208 { \u2212 1 , +1 } initialise : \u03b1 = 0 , n = 0 , \u03b2 lp = 0 u i = w + if y i = +1 w \u2212 otherwise where w + w \u2212 = \u03b2 and m i = 1 u i = 1 repeat n = n + 1 pass distribution d t to weak learner get weak hypothesis h n : x \u2192 r check for optimal solution : if m i = 1 u i y i h n ( x i ) \u2264 \u03b2 lp then n = n \u2212 1 , break solve restricted master for new costs : argmin ( u , \u03b2lp ) \u03b2 lp subject to m i = 1 u i y i h j ( x i ) \u2264 \u03b2 lp , j = 1 , . . . , n and m i = 1 u i = 1 and d + \u2264 u i \u2264 d + , if y i = +1 d \u2212 \u2264 u i \u2264 d \u2212 , otherwise , i = 1 , . . . , m end \u03b1 = lagrangian multipliers from last lp output the final hypothesis : f ( x ) = n j = 1 \u03b1 j h j ( x ) min \u03b1 , \u03c1 , \u03be + , \u03be \u2212 \u2212 \u03c1 + d \u03b2 m i = 1 \u03be + i + m i = 1 \u03be \u2212 i subject to y i n j = 1 \u03b1 j h j ( x i ) \u2265 \u03c1 \u2212 \u03be yi i , \u03be + i , \u03be \u2212 i \u2265 0 , i = 1 , . . . , m \u03b1 j \u2265 0 , j = 1 , . . . , n and m i = 1 \u03b1 i = 1 .", "u i ( d + d \u2212 = d + d \u2212 = \u03b2 ) .", "the choice of parameter d lb controls the lower bound on the misclassification cost u i .", "we could set it to 0 , but we rather set d \u2212 = 1 m\u03bd , where \u03bd \u2208 ( 0 , 1 ) , andd \u2212 d \u2212 = d + d + = d lb .", "the problem we observed is the sensitivity of lp and lpu boost to the value of parameter \u03bd , which has to be tuned with some care to obtain a good convergence rate.lpuboost combines adauboost having an uneven loss function and lpboost having a well - defined stopping criterion with a mechanism to prevent overfitting .", "this makes lpuboost a good algorithm for learning on uneven training sets : maintaining higher weights on positive examples while using an lp to obtain an optimal combination of weak hypotheses with respect to a well - motivated optimisation criterion .", "a weak learning algorithm is a procedure for computing weak hypotheses .", "boosting finds a set of weak hypotheses by repeatedly calling a weak learning algorithm .", "the weak hypotheses are linearly combined into a single rule .", "the input of the weak learning algorithm is a distribution or vector of weights ( misclassification costs ) , and a training data set .", "weak learning algorithms use the weights to find a weak hypothesis which has a moderately low error with respect to the weights.due to the weight updating rule examples which are hard to classify will get incrementally higher weights while the examples easy to classify get lower weights .", "the effect is to force the subsequent weak learners to concentrate on hard - to - classify examples .", "boosting is a general purpose method that can be combined with any weak learner .", "in practice it has been combined with a wide variety of classes including decision trees and neural networks.in this paper we focus on boosting using very simple classifiers .", "all classifiers considered in this section have the form of a one level decision tree ( if - then rule ) .", "the test is to check for the presence of a word in a given document .", "based on the presence of the word the weak hypothesis outputs a prediction .", "we interpret the sign of the prediction as a predicted class ( recall we are dealing with binary problems ) and the magnitude of the output as the confidence in that prediction.for example : if we try to predict which documents belong to the sports category , we will train a classifier to make a distinction between sports documents ( positive class ) and the rest of the documents ( negative class ) .", "then our weak hypothesis could be a rule : \" if the word football occurs in a document , then we are highly confident the document belongs to sports category .", "on the other hand , if football does not occur in a document then we predict the document does not belong to the sports category with low confidence .", "\" formally , we write w \u2208 x to mean a term w occurs in document x .", "so we define a weak hypotheses h which makes predictions : h ( x ) = c + if w \u2208 x c \u2212 if w / \u2208 x ( 2 ) where c + and c \u2212 are real numbers.let us also define : given a current distribution d t and a term w : let x + be a set of documents having the term w , x + = { x : w \u2208 x } , x \u2212 = { x : w / \u2208 x } and b , l \u2208 { + , \u2212 } then we calculate w has \u2212 word class : w b l = xi \u2208 x b \u2227 yi = l d t ( i ) ( 3 ) the weak learners presented in this subsection all have the same form of weak hypotheses , but they impose different restrictions on the values c + , c \u2212 and \u03b1 t and use different criterion to choose a weak hypothesis at each round of learning.at each round of learning our weak learners search all possible terms .", "for each term , the values c + and c \u2212 and a score are assigned to that particular weak learner .", "after all terms have been searched , a learner with best score is chosen .", "different weak learners use different scores.for adaboost.mh a bound on empirical loss ( fraction of misclassified examples ) has been proven by schapire and singer [ 11 ] .", "they showed that the hamming loss ( mh stands for minimum hamming loss ) of the boosted function obtained using adaboost.mh is at most t t = 1 z t , where z t is a normalization factor at round t .", "this upper bound can be a used as a guideline for choosing \u03b1 t and the design of the weak learning algorithm .", "the first algorithm is called adaboost.mh ( real.mh ) with real - valued predictions [ 12 ] .", "we permit c + and c \u2212 to be unrestricted real valued predictions.it was shown in [ 11 ] that z t is minimised by choosingc b = 1 2 ln w b + w b \u2212 ( 4 ) and setting \u03b1 t = 1 implies that : z t = 2 w + + w + \u2212 + w \u2212 + w \u2212 \u2212 ( 5 ) thus we choose the term w for which z t has the minimal value .", "as suggested in schapire and singer [ 12 ] we smooth the values of c b to limit the magnitudes of predictionsc b = 1 2 ln w b + + w b \u2212 + ( 6 ) we use = 1 m .", "since w b + and w b \u2212 \u2208 [ 0 , 1 ] , this bounds | c b | by roughly 1 2 ln ( 1 / / ) .", "adaboost.mh with discrete predictions ( disc.mh ) forces the predictions c b of the weak hypothesis to be either +1 or \u2212 1 .", "this is a more traditional setting where predictions do not carry confidences , and \u03b1 t is a measure of confidence in the weak hypothesis .", "we still minimise z t for a given term w. using the same notation as introduced in the previous section , we set : c b = sign ( w b + \u2212 w b \u2212 ) ( 7 ) we can interpret the choice of c b as a weighted majority vote over training examples .", "let r t = | w + + \u2212 w + \u2212 | + | w \u2212 + \u2212 w \u2212 \u2212 | , then it can be shown [ 11 ] , that in order to minimise z t we should choose : \u03b1 t = 1 2 ln 1 + r t 1 \u2212 r t ( 8 ) giving z t = 1 \u2212 r 2 t .", "so we choose a weak hypothesis ( a term w ) which has the smallest z t .", "the lpboost algorithm [ 5 ] ( see section 2.3 ) suggests that at each round of boosting a weak hypothesis h with maximal sum of misclassification costs d t multiplied by class value y i \u2208 { \u2212 1 , +1 } and prediction h : dtsum = m i = 1 d t ( i ) h ( x i ) y i ( 9 ) should be chosen .", "since this is a different criterion for choosing the best weak - hypothesis in adaboost.mh , we obtain two new weak learners .", "we call them real.lp and disc.lp .", "they differ from adaboost.mh in the way weak hypothesis is chosen , instead of minimising z t , dtsum is maximised .", "karakoulas and shawe - taylor in their paper on boosting imbalanced training sets [ 7 ] proposed a new method for calculating z t and choosing \u03b1 t .", "note that in case of an uneven loss function ( adauboost , lpuboost ) we have an additional parameter \u03b2 .", "positive examples will get \u03b2 times higher weight than negative examples .", "the parameter \u03b2 will also guide the weight updating rule to increase the weight of false negatives more aggressively than of false positives .", "we define z t as : z t ( \u03b1 t ) = m i = 1 d t ( i ) exp ( \u2212 \u03b1 t \u03b2 i h t ( x i ) y i ) ( 10 ) where \u03b2 i = 1 / \u03b2 if y i = +1 and 1 if otherwise .", "to minimise the error we seek to minimise z t with respect to \u03b1 t .", "by taking the first derivative of ( 10 ) and equating it to zero and introducing notationw c , p = m i = 1 d t ( i ) | y i = c \u2227 h ( x i ) = p , where c , p \u2208 { \u2212 1 , +1 } , we get : \u2212 exp ( \u2212 \u03b1 t / \u03b2 ) w + + / \u03b2 + exp ( \u03b1 t / \u03b2 ) w \u2212 + / \u03b2 + exp ( \u03b1 t ) w + \u2212 \u2212 exp ( \u2212 \u03b1 t ) w \u2212 \u2212 = 0 .", "substituting y = exp ( \u03b1 t ) we obtain : c 1 y 1 \u2212 1 / \u03b2 + c 2 y 1 +1 / \u03b2 + c 3 y 2 + c 4 = 0 ( 11 ) wherec 1 = \u2212 w + + / \u03b2 , c 2 = w \u2212 + / \u03b2 , c 3 = w + \u2212 , c 4 = \u2212 w \u2212 \u2212 .", "the root of equation ( 11 ) can be found numerically .", "z t ( \u03b1 t ) > 0 implies z t ( \u03b1 t ) is convex and has only one minimum.a weak hypothesis h with minimal z t is chosen .", "this is another way of calculating \u03b1 t and two new weak learners can be obtained ( real.u , disc.u ) .", "they differ from adaboost.mh only in the way z t is calculated and for discrete learners we set \u03b1 t to be the solution of ( 11 ) which minimises z t .", "the following sections describe the experimental setup .", "we also describe and analyse experiments performed using the four boosting algorithms and six text categorization weak learners that were described in the previous sections .", "we performed empirical evaluation on the modapte split of the reuters - 21578 dataset compiled by david lewis .", "the split consists of 12 , 902 documents of which 9 , 603 are used for training and 3 , 299 for testing.the following preprocessing was performed : all words were converted to lower case and punctuation marks ( 538 ) were removed .", "we removed stop words from a list of 523 english words .", "we used the porter stemmer [ 9 ] and retained only those terms having document frequency larger than 3 .", "after the preprocessing the corpus contained 6 , 242 distinct features ( terms ) .", "we used the sv m light [ 6 ] implementation of svm with a linear kernel .", "based on category size we have chosen a set of 16 reuters - 21578 categories .", "some of them are large ( earn , acq ) and some really small ( potato , platinum ) having only a few examples.we trained boosting binary classifiers to make predictions whether a document belongs to a category or not .", "we assigned all documents having the category a positive class and all other documents a negative class.we ran a set of 120 experiments for a single reuters category using combinations of all the described boosting algorithms and weak learners .", "in all experiments a number of rounds of learning was set to 300 .", "we tested the combinations of the following parameters : \u03bd : 0.1 , 0.2 ; d lb : 0 , 10 , 50 , 100 ; \u03b2 : 2 , 4 , 8 .", "for each boosting algorithm we display the best experiment using the standard information retrieval f1 score on the test dataset .", "we realise that choosing the best performance on the test set invalidates their status .", "but the aim of this experiment was to get the idea about the best possible performance ( upper bound ) of various algorithms .", "table 1 shows results for chosen categories .", "as we can see from the averages lpuboost ( lpu ) is dominant , followed by the lpboost ( lp ) and adauboost ( u ) .", "adaboost ( ada ) and linear svm are far behind .", "on large categories ada , u and svm are a little better than lp and lpu , but as we move to smaller categories the qualities of lpu ( and also lp and u to some extent ) seem to appear .", "adauboost has the feature of uneven loss function which helps and lpboost has the mechanism to find an optimal combination of weak hypotheses ; both these features are combined in lpuboost.on small categories ada and u overfit the training data ( figure 3 ) .", "adauboost is more resistant to overfitting than adaboost .", "on the other hand the performance of lp and lpu on the training set decreases with decreasing category size , but on the test set it remains at about the same level .", "we can see that lp and lpu are superior to ada and svm while u does a little worse than lp and lpu.secondly we performed the same set of 120 experiments using stratified 5 fold cross validation .", "based on average f1 score over 5 trials , we have chosen best parameter configuration for each algorithm .", "table 2 shows the average f1 on test set.we can see that results obtained by cross validation ( table 2 ) are not far from the optimal ( table 1 ) .", "for algorithms with a small set of parameters ( adaboost , svm ) the difference between optimal and cross validation performance is small .", "there is a surprisingly large gap for u and even larger for lp.svm performs best of all algorithms on categories with more than 1 % of positive training examples , but as we decrease the category size it is no more competitive .", "lpuboost performs best and not far from optimal .", "we also observed that optimal parameter settings are different from those obtained by cross validation .", "performance of lpuboost is quite stable on the whole range of categories of various sizes .", "figure 4 shows typical learning curves of boosting algorithms .", "in the top row we see the typical performance of adaboost on large categories and the oscillations we noticed in adauboost .", "the bottom row shows lp and lpu .", "we observe larger jumps in performance than for instance with ada or u. typically at the early rounds of learning lp ( lpu ) is not stable , but when we move forward performance gets more stable , when finally the algorithm converges .", "lp and lpu use many fewer weak hypotheses than ada or u .", "we trained ada and u for 300 rounds ( 300 weak hypotheses were chosen ) .", "lp and lpu converged in around 50 rounds for large and around 5 or less rounds of learning for small categories .", "for some of the smallest categories lpu picked just 1 or 2 weak hypotheses and made no error - for category platinum it is sufficient to check for the presence of a word platinum in order to make perfect predictions.considering chosen weak learners we see real.mh gives best performance for most of the categories .", "this is in accordance with the results reported in [ 12 ] .", "real.mh is followed by disc.mh and real.lp .", "special weak learners did not improve the performance - not even in combination with the boosting algorithm they were designed to work together .", "we took 2 largest categories : earn and acq .", "training set consists of all negative and a number of randomly selected positive training examples .", "by selecting a number of positive training examples we artificially created a small category.we performed the same set of 120 experiments as in section 4.2 .", "for each boosting algorithm we display the best experiment using the f1 score on the test set .", "this means we show the upper bound of the algorithm .", "figure 5 shows the results which are surprising .", "previous experiments showed that lpu is best on uneven training sets , because it does not overfit and picks a sufficiently small number of weak learners .", "but figure 5 shows a different picture .", "as the number of positive training examples decreases , the performance of lpu ( lp ) also dramatically decrease , but the performance of u ( ada ) remains almost at the same level.we observed almost the same things with acq : u is still the best , closely followed by lpu .", "performance of ada and lp is poor and after the number of positive training examples drops bellow 50 , f1 is less than 0.1 .", "since experiment showed that the best possible performance of lp and lpu are far behind from ada and u , we did n't run stratified cross validation.lpu ( lp ) performed very well on naturally uneven datasets .", "but when we artificially create an small category , the performance of lpu decreased dramatically.this suggests that there is a fundamental difference between naturally small and artificially small categories .", "we think that reuters ' editors categorizing documents made earn very diverse ( broad and not specific ) , while a small category like platinum is very specific .", "note that the test set is unchanged so it resembles original category ( has the same distribution as original category ) .", "to make good predictions using small training ( and large diverse testing ) earn one has to \" overfit \" by taking all ( not necessarily significant ) features of the training data .", "on the other hand we have to be very careful and take only really significant features to make good predictions on platinum .", "this paper introduces lpuboost boosting algorithm .", "we provide both theoretical and empirical evidence that lpuboost is well suited for text categorization for uneven data sets.lpuboost has many benefits over gradient - based approaches : finite termination at globally optimal solution , well - defined convergence criteria , unequal loss function and use fewer weak hypotheses ."], "tgt_txt": "icml 2003 1 linear programming boosting for uneven datasets jurij leskovec , joef stefan institute , slovenia john shawe - taylor , royal holloway university of london , uk icml 2003 2 motivation there are 800 million of europeans and 2 million of them are slovenians want to build a classifier to distinguish slovenians from the rest of europeans a traditional unaware classifier ( e.g. politician ) would not even notice slovenia as an entity we do nt want that !<q>icml 2003 3 problem setting unbalanced dataset 2 classes : positive ( small ) negative ( large ) train a binary classifier to separate highly unbalanced classes icml 2003 4 our solution framework we will use boosting combine many simple and inaccurate categorization rules ( weak learners ) into a single highly accurate categorization rule the simple rules are trained sequentially ; each rule is trained on examples which are most difficult to classify by preceding rules icml 2003 5 outline boosting algorithms weak learners experimental setup results conclusions icml 2003 6 related approaches : adaboost given training examples ( x1 , y1 ) , ( xm , ym ) initialize d0 ( i ) = 1 / m yi { +1 , -1 } for t = 1t pass distribution dt to weak learner get weak hypothesis ht : x r choose t ( based on performance of ht ) update dt +1 ( i ) = dt ( i ) exp ( - t yi ht ( xi ) ) / zt final hypothesis : f ( x ) = t t ht ( x ) icml 2003 7 adaboost - intuition weak hypothesis h ( x ) sign of h ( x ) is the predicted binary label magnitude | h ( x ) | as a confidence t controls the influence of each ht ( x ) icml 2003 8 more boosting algorithms algorithms differ in the way of initializing weights d0 ( i ) ( misclassification costs ) and updating them 4 boosting algorithms : adaboost greedy approach uboost uneven loss function + greedy lpboost linear programming ( optimal solution ) lpuboost our proposed solution ( lp + uneven ) icml 2003 9 given training examples ( x1 , y1 ) , ( xm , ym ) initialize d0 ( i ) = 1 / m yi { +1 , -1 } for t = 1t pass distribution dt to weak learner get weak hypothesis ht : x r choose t update dt +1 ( i ) = dt ( i ) exp ( - t yi ht ( xi ) ) / zt final hypothesis : f ( x ) = t t ht ( x ) boosting algorithm differences boosting algorithms differ in these 2 lines icml 2003 10 uboost - uneven loss function set : d0 ( i ) so that d0 ( positive ) / d0 ( negative ) = update dt +1 ( i ) : increase weight of false negatives more than on false positives decrease weight of true positives less than on true negatives positive examples maintain higher weight ( misclassification cost ) icml 2003 11 lpboost linear programming set : d0 ( i ) = 1 / m update dt +1 : solve lp : argmin lpbeta , s.t. i ( d ( i ) yi hk ( xi ) ) lpbeta ; k = 1t where 1 / a < d ( i ) < 1 / b set to lagrangian multipliers if i d ( i ) yi ht ( xi ) < lpbeta , optimal solution icml 2003 12 lpboost intuition argmin lpbeta s.t. i ( d ( i ) yi hk ( xi ) ) lpbeta k = 1 ... t where 1 / a < d ( i ) < 1 / b d ( 1 ) d ( 2 ) d ( 3 ) d ( m ) h1 + - + h2 - - + + lpbeta ht + - + + training example weights weak learners icml 2003 13 lpboost example d ( 1 ) d ( 2 ) d ( 3 ) h1 + 0.3 d ( 1 ) + 0.7 d ( 2 ) - 0.2 d ( 3 ) lpbeta h2 + 0.1 d ( 1 ) - 0.4 d ( 2 ) - 0.5 d ( 3 ) lpbeta h3 + 0.5 d ( 1 ) - 0.1 d ( 2 ) - 0.3 d ( 3 ) lpbeta training example weights argmin lpbeta s.t. i ( yi hk ( xi ) d ( i ) ) lpbeta k = 1 ... 3 where 1 / a < d ( i ) < 1 / b confidenceincorrectly classified correctly classified weak learners icml 2003 14 lpuboost - uneven loss + lp set : d0 ( i ) so that d0 ( positive ) / d0 ( negative ) = update dt +1 : solve lp , minimize lpbeta but set different misclassification cost bounds for d ( i ) ( times higher for positive examples ) the rest as in lpboost note : is input parameter .<q>lpbeta is linear programming optimization variable icml 2003 15 summary of boosting algorithms uneven loss function converges to global optimum adaboost uboost lpboost lpuboost icml 2003 16 weak learners one - level decision tree ( if - then rule ) : if word w occurs in a document x return p else return n p and n are real numbers chosen based on misclassification cost weights dt ( i ) interpret the sign of p and n as the predicted binary label magnitude | p | and | n | as the confidence icml 2003 17 experimental setup reuters newswire articles ( reuters recall = tp / ( tp + fn ) f1 = 2prec rec / ( prec + rec ) icml 2003 18 typical situations balanced training dataset all learning algorithms show similar performance unbalanced training dataset adaboost overfits lpuboost does not overfit converges fast using only a few weak learners uboost and lpboost are somewhere in between icml 2003 19 balanced dataset typical behavior icml 2003 20 unbalanced dataset adaboost overfits icml 2003 21 unbalanced dataset lpuboost few iterations ( 10 ) stop after no suitable feature is left icml 2003 22 reuters categories f1 on test set even uneven category ( size ) ada u lp lpu svm earn ( 2877 ) 0.97 0.97 0.97 0.91 0.98 acq ( 1650 ) 0.91 0.94 0.88 0.84 0.94 money - fx ( 538 ) 0.65 0.70 0.63 0.65 0.76 interest ( 347 ) 0.65 0.69 0.59 0.66 0.65 corn ( 181 ) 0.81 0.87 0.82 0.83 0.80 gnp ( 101 ) 0.78 0.80 0.64 0.66 0.81 carcass ( 50 ) 0.49 0.65 0.63 0.65 0.52 cotton ( 39 ) 0.68 0.89 0.95 0.95 0.68 meal - feed ( 30 ) 0.59 0.77 0.65 0.81 0.45 pet - chem ( 20 ) 0.03 0.16 0.03 0.19 0.17 lead ( 15 ) 0.20 0.67 0.24 0.45 0 soy - meal ( 13 ) 0.30 0.73 0.35 0.38 0.21 groundnut ( 5 ) 0 0 0.22 0.75 0 platinum ( 5 ) 0 0 0.20 1.00 0.32 potato ( 3 ) 0.53 0.53 0.29 0.86 0.15 naphtha ( 2 ) 0 0 0.20 0.89 0 average 0.47 0.59 0.52 0.72 0.46 icml 2003 23 lpuboost vs. uboost icml 2003 24 most important features ( stemmed words ) earn ( 2877 ) 50 : ct , net , profit , dividend , shr interest ( 347 ) 70 : rate , bank , company , year , pct carcass ( 50 ) 30 : beef , pork , meat , dollar , chicago soy - meal ( 13 ) 3 : meal , soymeal , soybean groundnut ( 5 ) 2 : peanut , cotton ( f1 = 0.75 ) platinum ( 5 ) 1 : platinum ( f1 = 1.0 ) potato ( 3 ) 1 : potato ( f1 = 0.86 ) category size lpu model size ( number of features / words ) icml 2003 25 computational efficiency adaboost and uboost are the fastest the simplest lpboost and lpuboost are a little slower lp computation takes much of the time but since lpuboost chooses fewer weak hypotheses the times get comparable to those of adaboost icml 2003 26 conclusions lpuboost is suitable for text categorization for highly unbalanced datasets all benefits ( well - defined stopping criteria , unequal loss function ) show up no overfitting : it is able to find simple ( small ) and complicated ( large ) hypotheses", "sections": [1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 11, 11, 11, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 15, 15, 15, 15, 15, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19], "token_sections": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]}